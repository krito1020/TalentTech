{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNh6NaFW4iHPVAZTKMQng5E",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EderLara/CuadernosPythonParaML/blob/main/Tutorial_Pandas_Elemental.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PANDAS**.\n",
        "\n",
        "[Pandas](https://pandas.pydata.org/) es una de las herramientas más fundamentales y poderosas para cualquier persona que trabaje con datos en Python. [Polars](https://pola.rs/) es una alternativa más nueva que está ganando mucha tracción por su rendimiento, pero entender Pandas primero te dará una base muy sólida.\n",
        "\n",
        "* Tutorial [Polars Elemental](https://colab.research.google.com/drive/1ytYKsVm17o5l6-m4UNviBaC8_KQrZymP?usp=sharing)\n",
        "---\n",
        "\n",
        "##1. **¿Qué es Pandas?**\n",
        "  * Pandas es una biblioteca de Python de código abierto que proporciona estructuras de datos de alto rendimiento y fáciles de usar, junto con herramientas de análisis de datos. Su nombre deriva de \"Panel Data\" (Datos de Panel), un término econométrico para conjuntos de datos estructurados multidimensionales.\n",
        "\n",
        "##2. **¿Por qué usar Pandas?**\n",
        "\n",
        "  * Manejo eficiente de datos tabulares: Es ideal para trabajar con datos organizados en filas y columnas, como los que encontrarías en una hoja de cálculo, una tabla de base de datos SQL o un archivo CSV.\n",
        "  * Limpieza y preparación de datos: Ofrece muchísimas funciones para limpiar datos desordenados (missing values, datos incorrectos, etc.) y transformarlos para el análisis.\n",
        "  * Análisis y exploración: Facilita el cálculo de estadísticas, la agrupación de datos, el filtrado, la selección y la exploración en general.\n",
        "  * Entrada/Salida (I/O): Permite leer y escribir datos fácilmente desde y hacia diversos formatos de archivo (CSV, Excel, JSON, SQL, etc.).\n",
        "  * Integración: Se integra muy bien con otras bibliotecas científicas de Python como NumPy, Matplotlib (para visualización) y Scikit-learn (para machine learning).\n",
        "\n",
        "##3. **Estructuras de Datos Principales:**\n",
        "\n",
        "Pandas introduce dos estructuras de datos principales que son la base de casi todo lo que harás:\n",
        "\n",
        "  * Series: Es un array unidimensional etiquetado capaz de contener cualquier tipo de dato (enteros, strings, números de punto flotante, objetos de Python, etc.). Piensa en ella como una sola columna de una hoja de cálculo o un diccionario ordenado. Cada elemento en una Series tiene una etiqueta asociada, llamada índice (index).\n",
        "\n",
        "  * DataFrame: Es una estructura de datos tabular bidimensional, similar a una hoja de cálculo de Excel, una tabla SQL o un diccionario de objetos Series. Un DataFrame tiene tanto un índice de filas como un índice de columnas. Es la estructura más utilizada en Pandas.\n",
        "\n",
        "---\n",
        "\n",
        "### **Instalación**\n",
        "\n",
        "Si necesitas instalarlo manualmente en tu entorno de Python, puedes hacerlo usando pip:\n",
        "\n",
        "```\n",
        "pip install pandas\n",
        "```\n",
        "\n",
        "### **Uso**\n",
        "Para usar Pandas en tus scripts, la convención es importarlo con el alias pd:\n",
        "\n",
        "```\n",
        "import pandas as pd\n",
        "import numpy as np # A menudo se usa junto con Pandas\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Q0k9f8a38tS0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Tabla de Contenido**\n",
        "### 1. Introducción y Estructuras de Datos: Series y DataFrame.\n",
        "### 2. Inspección Básica: head, tail, info, describe, shape, dtypes.\n",
        "### 3. Selección y Filtrado: [], .loc[], .iloc[], boolean indexing.\n",
        "### 4. Manipulación de Datos: Añadir/eliminar columnas, modificar valores, fillna, dropna, rename, astype, sort.\n",
        "### 5. Agrupación: groupby(), agg().\n",
        "### 6. Combinación: concat(), merge(), join().\n",
        "### 7. Entrada/Salida: read_csv(), read_excel(), to_csv(), to_excel().\n",
        "### 8. Funciones Avanzadas: Fechas con .dt, .plot(), duplicados, pivot_table, apply/map, texto con .str."
      ],
      "metadata": {
        "id": "0v_BrBgGAXi9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# **Creación de Series y DataFrames**\n",
        "\n",
        "## **Series:**\n",
        "\n",
        "Una Series se puede crear a partir de varias estructuras de datos de Python:\n",
        "\n",
        "  1. Desde una lista:\n",
        "  ```\n",
        "  import pandas as pd\n",
        "\n",
        "  # Creando una Serie desde una lista\n",
        "  datos_lista = [10, 20, 30, 40, 50]\n",
        "  serie_lista = pd.Series(datos_lista)\n",
        "\n",
        "  print(\"Serie desde una lista:\")\n",
        "  print(serie_lista)\n",
        "  \n",
        "  print(\"\\nÍndice de la serie:\", serie_lista.index)\n",
        "  print(\"Valores de la serie:\", serie_lista.values)\n",
        "  print(\"Tipo de datos:\", serie_lista.dtype)\n",
        "  # ------------------------------------------------------- #\n",
        "  # Salida Esperada:\n",
        "  # ------------------------------------------------------- #\n",
        "  Serie desde una lista:\n",
        "  0    10\n",
        "  1    20\n",
        "  2    30\n",
        "  3    40\n",
        "  4    50\n",
        "  dtype: int64\n",
        "\n",
        "  Índice de la serie: RangeIndex(start=0, stop=5, step=1)\n",
        "  Valores de la serie: [10 20 30 40 50]\n",
        "  Tipo de datos: int64\n",
        "  # ------------------------------------------------------- #\n",
        "  Pandas asigna automáticamente un índice numérico (0, 1, 2...) si no se especifica uno.\n",
        "  ```\n",
        "\n",
        "  2. Desde una lista con índice personalizado:\n",
        "  ```\n",
        "    import pandas as pd\n",
        "\n",
        "    datos_lista = [10, 20, 30]\n",
        "    indices_personalizados = ['a', 'b', 'c']\n",
        "    serie_con_indice = pd.Series(data=datos_lista, index=indices_personalizados)\n",
        "    print(\"\\nSerie con índice personalizado:\")\n",
        "    print(serie_con_indice)\n",
        "    print(\"Accediendo por etiqueta:\", serie_con_indice['b'])\n",
        "\n",
        "    # ------------------------------------------------------- #\n",
        "    # Salida Esperada:\n",
        "    # ------------------------------------------------------- #\n",
        "    Serie con índice personalizado:\n",
        "    a    10\n",
        "    b    20\n",
        "    c    30\n",
        "    dtype: int64\n",
        "    Accediendo por etiqueta: 20\n",
        "    # ------------------------------------------------------- #\n",
        "\n",
        "  ```\n",
        "\n",
        "3. Desde un diccionario:\n",
        "    * Las claves del diccionario se convierten en el índice y los valores del diccionario en los valores de la Series.\n",
        "    ```\n",
        "      import pandas as pd\n",
        "\n",
        "      datos_diccionario = {'Juan': 25, 'Ana': 30, 'Luis': 22, 'Sofia': 27}\n",
        "      serie_diccionario = pd.Series(datos_diccionario)\n",
        "      print(\"\\nSerie desde un diccionario:\")\n",
        "      print(serie_diccionario)\n",
        "      print(\"Edad de Ana:\", serie_diccionario['Ana'])\n",
        "    ```\n"
      ],
      "metadata": {
        "id": "PFYuyWs0AeQb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "QypuzN5hEfbs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Series con Pandas \"\"\"\n",
        "\n",
        "# Ejemplo 1: Serie con fechas como índice\n",
        "fechas = pd.date_range(start='2024-01-01', periods=5)\n",
        "valores = [10, 20, 15, 25, 30]\n",
        "serie_fechas = pd.Series(valores, index=fechas)\n",
        "print(\"Serie con fechas como índice:\\n\", serie_fechas)\n",
        "\n",
        "\n",
        "# Ejemplo 2: Serie con etiquetas de texto personalizadas\n",
        "etiquetas = ['A', 'B', 'C', 'D', 'E']\n",
        "serie_etiquetas = pd.Series(valores, index=etiquetas)\n",
        "print(\"\\nSerie con etiquetas de texto:\\n\", serie_etiquetas)\n",
        "\n",
        "# Ejemplo 3: Serie a partir de un diccionario\n",
        "diccionario = {'A': 10, 'B': 20, 'C': 15}\n",
        "serie_diccionario = pd.Series(diccionario)\n",
        "print(\"\\nSerie desde un diccionario:\\n\", serie_diccionario)\n",
        "\n",
        "# Ejemplo 4: Serie con valores mixtos y NaN (Not a Number)\n",
        "datos_mixtos = [10, 'Hola', 3.14, None, True]\n",
        "serie_mixta = pd.Series(datos_mixtos)\n",
        "print(\"\\nSerie con valores mixtos:\\n\", serie_mixta)\n",
        "print(\"\\nInformación de la serie mixta:\\n\", serie_mixta.info())\n",
        "\n",
        "# Ejemplo 5: Accediendo a elementos de la serie\n",
        "print(\"\\nAcceso por posición (índice numérico):\", serie_fechas[0])\n",
        "print(\"\\nAcceso por etiqueta:\", serie_etiquetas['C'])\n"
      ],
      "metadata": {
        "id": "jZJzpUpoEN3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Dataframes**\n",
        "Un DataFrame se puede crear de muchas maneras:\n",
        "\n",
        "  1. Desde un diccionario de listas o Series:\n",
        "    * Cada clave del diccionario se convierte en una columna, y las listas (o Series) asociadas a esas claves se convierten en los datos de esas columnas. Todas las listas/Series deben tener la misma longitud.\n",
        "    ```\n",
        "      import pandas as pd\n",
        "\n",
        "      datos_df = {\n",
        "          'Nombre': ['Ana', 'Luis', 'Marta', 'Juan'],\n",
        "          'Edad': [28, 34, 29, 42],\n",
        "          'Ciudad': ['Madrid', 'Barcelona', 'Valencia', 'Sevilla']\n",
        "      }\n",
        "      df_desde_dic_listas = pd.DataFrame(datos_df)\n",
        "      print(\"\\nDataFrame desde un diccionario de listas:\")\n",
        "      print(df_desde_dic_listas)\n",
        "\n",
        "      # ------------------------------------------------------- #\n",
        "      # Salida Esperada:\n",
        "      # ------------------------------------------------------- #\n",
        "      DataFrame desde un diccionario de listas:\n",
        "        Nombre  Edad     Ciudad\n",
        "      0    Ana    28     Madrid\n",
        "      1   Luis    34  Barcelona\n",
        "      2  Marta    29   Valencia\n",
        "      3   Juan    42    Sevilla\n",
        "    ```\n",
        "\n",
        "  2. Desde un diccionario de listas con índice personalizado:\n",
        "  ```\n",
        "    import pandas as pd\n",
        "\n",
        "    datos_df = {\n",
        "        'Manzanas': [3, 2, 0, 1],\n",
        "        'Naranjas': [0, 3, 7, 2]\n",
        "    }\n",
        "    indices_df = ['TiendaA', 'TiendaB', 'TiendaC', 'TiendaD']\n",
        "    df_con_indice = pd.DataFrame(datos_df, index=indices_df)\n",
        "    print(\"\\nDataFrame con índice personalizado:\")\n",
        "    print(df_con_indice)\n",
        "\n",
        "    # ------------------------------------------------------- #\n",
        "    # Salida Esperada:\n",
        "    # ------------------------------------------------------- #\n",
        "\n",
        "    DataFrame con índice personalizado:\n",
        "            Manzanas  Naranjas\n",
        "    TiendaA         3         0\n",
        "    TiendaB         2         3\n",
        "    TiendaC         0         7\n",
        "    TiendaD         1         2\n",
        "\n",
        "    ```\n",
        "  3. Desde una lista de diccionarios:\n",
        "    * Cada diccionario en la lista se convierte en una fila del DataFrame. Las claves de los diccionarios se convierten en los nombres de las columnas.\n",
        "    ```\n",
        "      import pandas as pd\n",
        "\n",
        "      lista_de_diccionarios = [\n",
        "          {'Nombre': 'Carlos', 'Edad': 25, 'Profesion': 'Ingeniero'},\n",
        "          {'Nombre': 'Laura', 'Edad': 30, 'Profesion': 'Doctora'},\n",
        "          {'Nombre': 'Pedro', 'Edad': 22} # Nota: Falta 'Profesion' aquí\n",
        "      ]\n",
        "      df_desde_lista_dic = pd.DataFrame(lista_de_diccionarios)\n",
        "      print(\"\\nDataFrame desde una lista de diccionarios:\")\n",
        "      print(df_desde_lista_dic)\n",
        "\n",
        "      # ------------------------------------------------------- #\n",
        "      # Salida Esperada:\n",
        "      # ------------------------------------------------------- #\n",
        "\n",
        "      DataFrame desde una lista de diccionarios:\n",
        "        Nombre  Edad  Profesion\n",
        "      0  Carlos    25  Ingeniero\n",
        "      1   Laura    30    Doctora\n",
        "      2   Pedro    22        NaN        # Pandas maneja la clave faltante (Profesion para Pedro) introduciendo un NaN (Not a Number), que es la forma estándar de Pandas para representar valores ausentes.\n",
        "    ```\n",
        "    \n",
        "  4. Desde un array de NumPy (con nombres de columna e índice opcionales):\n",
        "    ```\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "\n",
        "    array_np = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
        "    df_desde_numpy = pd.DataFrame(array_np, columns=['A', 'B', 'C'], index=['Fila1', 'Fila2', 'Fila3'])\n",
        "    print(\"\\nDataFrame desde un array de NumPy:\")\n",
        "    print(df_desde_numpy)\n",
        "\n",
        "    # ------------------------------------------------------- #\n",
        "    # Salida Esperada:\n",
        "    # ------------------------------------------------------- #\n",
        "    DataFrame desde un array de NumPy:\n",
        "           A  B  C\n",
        "    Fila1  1  2  3\n",
        "    Fila2  4  5  6\n",
        "    Fila3  7  8  9\n",
        "    ```"
      ],
      "metadata": {
        "id": "Vr7Ap7fvN0xa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0if8TAf88stV"
      },
      "outputs": [],
      "source": [
        "\"\"\" Creando Dataframes desde una lista \"\"\"\n",
        "\n",
        "# Ejemplo 1: DataFrame a partir de una lista de listas\n",
        "data = [['Alice', 25, 'Ingeniera'],\n",
        "        ['Bob', 30, 'Doctor'],\n",
        "        ['Charlie', 28, 'Abogada']]\n",
        "df1 = pd.DataFrame(data, columns=['Nombre', 'Edad', 'Profesión'])\n",
        "print(\"DataFrame 1:\\n\", df1)\n",
        "\n",
        "# Ejemplo 2: DataFrame a partir de una lista de diccionarios\n",
        "data = [{'Nombre': 'Alice', 'Edad': 25, 'Profesión': 'Ingeniera'},\n",
        "        {'Nombre': 'Bob', 'Edad': 30, 'Profesión': 'Doctor'},\n",
        "        {'Nombre': 'Charlie', 'Edad': 28, 'Profesión': 'Abogada'}]\n",
        "df2 = pd.DataFrame(data)\n",
        "print(\"\\nDataFrame 2:\\n\", df2)\n",
        "\n",
        "\n",
        "# Ejemplo 3: DataFrame con índice personalizado a partir de una lista de listas\n",
        "data = [['Alice', 25, 'Ingeniera'],\n",
        "        ['Bob', 30, 'Doctor'],\n",
        "        ['Charlie', 28, 'Abogada']]\n",
        "\n",
        "index_personalizado = ['A', 'B', 'C'] # Índice personalizado\n",
        "\n",
        "df3 = pd.DataFrame(data, index=index_personalizado, columns=['Nombre', 'Edad', 'Profesión'])\n",
        "print(\"\\nDataFrame 3 (con índice personalizado):\\n\", df3)\n",
        "\n",
        "\n",
        "# Ejemplo 4: DataFrame a partir de una lista de tuplas\n",
        "data = [('Alice', 25, 'Ingeniera'),\n",
        "        ('Bob', 30, 'Doctor'),\n",
        "        ('Charlie', 28, 'Abogada')]\n",
        "df4 = pd.DataFrame(data, columns=['Nombre', 'Edad', 'Profesión'])\n",
        "print(\"\\nDataFrame 4 (a partir de tuplas):\\n\", df4)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# **Inspección y Exploración Básica de Datos.**\n",
        "\n",
        "Pandas ofrece varios métodos y atributos muy útiles para la inspección y exploración básica de datos.\n",
        "\n",
        "* Creación de un dataset (También podemos cargar desde el explorador de archivos):"
      ],
      "metadata": {
        "id": "zYEZX6yKMIfr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np                    # Lo usaremos para generar algunos datos y NaN\n",
        "\n",
        "# Creando un DataFrame de ejemplo más completo\n",
        "datos_ejemplo = {\n",
        "    'ID_Producto': [101, 102, 103, 104, 105, 106, 107, 108],\n",
        "    'Nombre_Producto': ['Manzana', 'Banana', 'Naranja', 'Manzana', 'Uva', 'Pera', 'Banana', 'Kiwi'],\n",
        "    'Categoría': ['Fruta', 'Fruta', 'Fruta', 'Fruta', 'Fruta', 'Fruta', 'Fruta', 'Fruta'],\n",
        "    'Precio_Unitario': [1.2, 0.5, 0.8, 1.2, 2.5, 1.5, 0.6, np.nan], # Incluimos un NaN\n",
        "    'Stock_Disponible': [100, 150, np.nan, 80, 60, 120, 140, 75],  # Incluimos un NaN\n",
        "    'Fecha_Ingreso': pd.to_datetime(['2024-01-10', '2024-01-11', '2024-01-10', '2024-01-12',\n",
        "                                   '2024-01-13', '2024-01-11', '2024-01-14', '2024-01-15']),\n",
        "    'En_Oferta': [False, True, False, False, True, True, False, True]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(datos_ejemplo)\n",
        "\n",
        "print(\"DataFrame de ejemplo creado:\")\n",
        "print(df)\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")           # Separador visual"
      ],
      "metadata": {
        "id": "fDnVNlBgMbsO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Herramientas de Inspección**\n",
        "\n",
        "* `head(n)` y `tail(n)`: Ver las primeras/últimas filas:\n",
        "  * Estos métodos te permiten ver una muestra de tus datos, por defecto las primeras 5 filas con head() y las últimas 5 con tail(). Puedes pasar un número n como argumento para ver una cantidad diferente de filas."
      ],
      "metadata": {
        "id": "cD_JmGpkMoUl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Primeras 3 filas del DataFrame (df.head(3)):\")\n",
        "print(df.head(3))\n",
        "\n",
        "print(\"\\nÚltimas 2 filas del DataFrame (df.tail(2)):\")\n",
        "print(df.tail(2))\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "id": "rdt3Z7IoMmlh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* `shape`: Dimensiones del DataFrame:\n",
        "  * Es un atributo (no un método, por eso no lleva paréntesis) que devuelve una tupla con el número de filas y el número de columnas."
      ],
      "metadata": {
        "id": "8mfu0027NKjU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Dimensiones del DataFrame (filas, columnas) (df.shape):\")\n",
        "print(df.shape)\n",
        "print(f\"El DataFrame tiene {df.shape[0]} filas y {df.shape[1]} columnas.\")\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "id": "SeMNjj-HNkjH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* `info()`: Resumen conciso del DataFrame\n",
        "  * Este método es extremadamente útil. Proporciona información sobre el índice, las columnas, los tipos de datos de cada columna, la cantidad de valores no nulos y el uso de memoria."
      ],
      "metadata": {
        "id": "SoeE99qHNmSg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Información concisa del DataFrame (df.info()):\")\n",
        "df.info()\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "id": "QP-T5_GJObbz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "De `info()` aprendemos rápidamente:\n",
        "\n",
        "* Hay 8 filas (entradas).\n",
        "* Los nombres de las columnas y cuántos valores no nulos tiene cada una (esto ayuda a identificar columnas con datos faltantes, como Precio_Unitario y Stock_Disponible).\n",
        "* El tipo de dato (Dtype) de cada columna (object usualmente significa string, datetime64[ns] para fechas, etc.)."
      ],
      "metadata": {
        "id": "53OQTHAWOk-s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* `describe()`: Estadísticas descriptivas\n",
        "  * Este método genera estadísticas descriptivas para las columnas numéricas por defecto (count, mean, std, min, 25th percentile, median (50th), 75th percentile, max)."
      ],
      "metadata": {
        "id": "XnOpsQU9OdiW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Estadísticas descriptivas para columnas numéricas (df.describe()):\")\n",
        "print(df.describe())\n",
        "\n",
        "# Para incluir estadísticas de columnas no numéricas (como strings/objects):\n",
        "print(\"\\nEstadísticas descriptivas para columnas de tipo 'object' (df.describe(include='object')):\")\n",
        "print(df.describe(include='object'))\n",
        "\n",
        "# Para incluir todas las columnas (numéricas y no numéricas):\n",
        "print(\"\\nEstadísticas descriptivas para TODAS las columnas (df.describe(include='all')):\")\n",
        "print(df.describe(include='all', datetime_is_numeric=True)) # datetime_is_numeric para tratar fechas\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "\"\"\"\n",
        "Para columnas object, describe() muestra count (conteo), unique (valores únicos), top (valor más frecuente) y freq (frecuencia del valor más frecuente).\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "8nLhtn6SO2T6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* `dtypes`: Tipos de datos de cada columna\n",
        "  * Es un atributo que devuelve una Serie con el tipo de dato de cada columna."
      ],
      "metadata": {
        "id": "lDnBM20oPHaM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Tipos de datos de cada columna (df.dtypes):\")\n",
        "print(df.dtypes)\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "id": "q-AUdCZfPGfO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* `columns`: Nombres de las columnas\n",
        "  * Un atributo que devuelve un objeto Index con los nombres de todas las columnas."
      ],
      "metadata": {
        "id": "lBV0CVa0QK2H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Nombres de las columnas (df.columns):\")\n",
        "print(df.columns)\n",
        "print(\"Lista de nombres de columnas:\", list(df.columns))\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "id": "8vHLG4uUO5JT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* `index:` Índice del DataFrame\n",
        "  * Un atributo que devuelve el objeto Index de las filas."
      ],
      "metadata": {
        "id": "KdKdX4_bQUUg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Índice del DataFrame (df.index):\")\n",
        "print(df.index)\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "\"\"\"\n",
        "En este caso, es un RangeIndex porque no especificamos uno personalizado al crear el DataFrame.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "7GZ4UakQRArg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* `value_counts()`: Frecuencia de valores en una Serie (columna)\n",
        "  * Este método se aplica a una Serie (una columna de un DataFrame) y cuenta cuántas veces aparece cada valor único. Es muy útil para columnas categóricas."
      ],
      "metadata": {
        "id": "kNH-u6SIRh9X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Frecuencia de valores en la columna 'Nombre_Producto' (df['Nombre_Producto'].value_counts()):\")\n",
        "print(df['Nombre_Producto'].value_counts())\n",
        "\n",
        "print(\"\\nFrecuencia de valores en la columna 'En_Oferta' (df['En_Oferta'].value_counts()):\")\n",
        "print(df['En_Oferta'].value_counts())\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "id": "sUTkZflKReI6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* `nunique()`: Número de valores únicos\n",
        "  * Se puede aplicar a una Serie para obtener el número de valores distintos en esa columna, o a un DataFrame para obtenerlo por cada columna."
      ],
      "metadata": {
        "id": "XKyy4wVoRsu5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Número de valores únicos en la columna 'Nombre_Producto' (df['Nombre_Producto'].nunique()):\")\n",
        "print(df['Nombre_Producto'].nunique())\n",
        "\n",
        "print(\"\\nNúmero de valores únicos por cada columna (df.nunique()):\")\n",
        "print(df.nunique())\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "id": "EfYjNQNlRrsy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Selección y Filtrado de Datos**\n",
        "\n",
        "En esta sección usaremos el mismo dataframe:\n",
        "\n",
        "\n",
        "```\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# DataFrame de ejemplo (el mismo del Paso 2)\n",
        "datos_ejemplo = {\n",
        "    'ID_Producto': [101, 102, 103, 104, 105, 106, 107, 108],\n",
        "    'Nombre_Producto': ['Manzana', 'Banana', 'Naranja', 'Manzana', 'Uva', 'Pera', 'Banana', 'Kiwi'],\n",
        "    'Categoría': ['Fruta', 'Fruta', 'Fruta', 'Fruta', 'Fruta', 'Fruta', 'Fruta', 'Fruta'],\n",
        "    'Precio_Unitario': [1.2, 0.5, 0.8, 1.2, 2.5, 1.5, 0.6, np.nan],\n",
        "    'Stock_Disponible': [100, 150, np.nan, 80, 60, 120, 140, 75],\n",
        "    'Fecha_Ingreso': pd.to_datetime(['2024-01-10', '2024-01-11', '2024-01-10', '2024-01-12',\n",
        "                                   '2024-01-13', '2024-01-11', '2024-01-14', '2024-01-15']),\n",
        "    'En_Oferta': [False, True, False, False, True, True, False, True]\n",
        "}\n",
        "df = pd.DataFrame(datos_ejemplo)\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "xSS8cRTESdEs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"DataFrame original:\")\n",
        "print(df)\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "id": "M2kW4KRCTbJq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Selección de columnas**\n",
        "  * Seleccionar una sola columna (devuelve una Serie):\n",
        "    * Puedes usar corchetes [ ] con el nombre de la columna:"
      ],
      "metadata": {
        "id": "anoPcg_bTb-m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Seleccionar la columna 'Nombre_Producto'\n",
        "nombres_productos = df['Nombre_Producto']\n",
        "\n",
        "print(\"Columna 'Nombre_Producto' (es una Serie):\")\n",
        "print(nombres_productos)\n",
        "print(\"Tipo de datos de la selección:\", type(nombres_productos))\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "id": "LDdOxz1mTwky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Seleccionar múltiples columnas (devuelve un DataFrame):\n",
        "    * Usa una lista de nombres de columnas dentro de los corchetes [[ ]]."
      ],
      "metadata": {
        "id": "7rZ_vZUyULy1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Seleccionar las columnas 'Nombre_Producto' y 'Precio_Unitario'\n",
        "seleccion_columnas = df[['Nombre_Producto', 'Precio_Unitario', 'Stock_Disponible']]\n",
        "\n",
        "print(\"DataFrame con columnas seleccionadas:\")\n",
        "print(seleccion_columnas)\n",
        "print(\"Tipo de datos de la selección:\", type(seleccion_columnas))\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "id": "qrkSrvEyUSax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Seleccionar usando notación de punto . (para una sola columna):\n",
        "    * Si el nombre de la columna es un identificador válido de Python (sin espacios, no empieza con número, no es una palabra clave, etc.), puedes usar la notación de punto. Es más conciso, pero menos flexible."
      ],
      "metadata": {
        "id": "81yFLiBAUYbP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Seleccionar la columna 'Categoría' usando notación de punto\n",
        "categorias = df.Categoría     # Equivalente a df['Categoría']\n",
        "\n",
        "print(\"Columna 'Categoría' usando notación de punto:\")\n",
        "print(categorias)\n",
        "# ¡Cuidado! df.ID_Producto funcionaría, pero si la columna se llamara \"ID Producto\" (con espacio), no.\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "id": "hba3V5TLUhiP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Selección de Filas (y Columnas) con loc e iloc**\n",
        "\n",
        "Pandas proporciona dos métodos principales para la selección basada en índices:\n",
        "\n",
        "  * df.loc[]: Se basa en etiquetas (labels) del índice y nombres de columnas.\n",
        "  * df.iloc[]: Se basa en la posición entera (integer position) del índice y columnas (desde 0).\n",
        "\n",
        "Nuestro DataFrame df actual tiene un RangeIndex (0, 1, 2,... 7). En este caso, las etiquetas del índice son los enteros de la posición, por lo que loc e iloc pueden parecer similares para la selección simple de filas, ¡pero conceptualmente son diferentes! `loc` usa la etiqueta 3, mientras `iloc` usa la fila en la posición 3.\n",
        "\n",
        "  * **Usando `loc` (selección basada en etiquetas):**"
      ],
      "metadata": {
        "id": "AXF-xWZZUhEN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- Selección con loc ---\")\n",
        "# Seleccionar una fila por su etiqueta de índice\n",
        "fila_con_etiqueta_2 = df.loc[2] # Fila con índice (label) 2\n",
        "print(\"\\nFila con etiqueta de índice 2:\")\n",
        "print(fila_con_etiqueta_2) # Devuelve una Serie\n",
        "\n",
        "# Seleccionar múltiples filas por sus etiquetas de índice\n",
        "filas_etiquetas_0_3_5 = df.loc[[0, 3, 5]]\n",
        "print(\"\\nFilas con etiquetas de índice 0, 3 y 5:\")\n",
        "print(filas_etiquetas_0_3_5) # Devuelve un DataFrame\n",
        "\n",
        "# \"Slicing\" de filas por etiquetas de índice (¡OJO! con loc, el final es INCLUIDO)\n",
        "slice_filas_loc = df.loc[1:4] # Filas desde etiqueta 1 HASTA etiqueta 4 (inclusive)\n",
        "print(\"\\nSlice de filas por etiqueta [1:4]:\")\n",
        "print(slice_filas_loc)\n",
        "\n",
        "# Seleccionar un valor específico (escalar) por etiqueta de fila y nombre de columna\n",
        "precio_producto_103 = df.loc[2, 'Precio_Unitario'] # Fila con etiqueta 2, columna 'Precio_Unitario'\n",
        "print(f\"\\nPrecio del producto en fila con etiqueta 2: {precio_producto_103}\")\n",
        "\n",
        "# Seleccionar todas las filas para columnas específicas\n",
        "nombres_y_precios = df.loc[:, ['Nombre_Producto', 'Precio_Unitario']]\n",
        "print(\"\\nTodas las filas, columnas 'Nombre_Producto' y 'Precio_Unitario':\")\n",
        "print(nombres_y_precios)\n",
        "\n",
        "# Seleccionar un subconjunto de filas y columnas por etiquetas\n",
        "subconjunto_loc = df.loc[0:2, ['Nombre_Producto', 'Categoría', 'Precio_Unitario']] # Filas 0,1,2 y esas 3 columnas\n",
        "print(\"\\nSubconjunto con loc (filas 0-2, columnas específicas):\")\n",
        "print(subconjunto_loc)\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "id": "0RBLF_SEbYMt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "  * **Usando `iloc` (selección basada en posición entera):**\n",
        "    * Siempre usa enteros para las posiciones, empezando en 0."
      ],
      "metadata": {
        "id": "lwy6sAeCbas6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- Selección con iloc ---\")\n",
        "# Seleccionar una fila por su posición entera\n",
        "fila_posicion_1 = df.iloc[1] # La segunda fila (posición 1)\n",
        "print(\"\\nFila en posición 1:\")\n",
        "print(fila_posicion_1) # Devuelve una Serie\n",
        "\n",
        "# Seleccionar múltiples filas por sus posiciones enteras\n",
        "filas_posiciones_0_3_5 = df.iloc[[0, 3, 5]]\n",
        "print(\"\\nFilas en posiciones 0, 3 y 5:\")\n",
        "print(filas_posiciones_0_3_5) # Devuelve un DataFrame\n",
        "\n",
        "# \"Slicing\" de filas por posiciones enteras (¡OJO! con iloc, el final es EXCLUIDO, como en Python)\n",
        "slice_filas_iloc = df.iloc[1:4] # Filas desde posición 1 HASTA posición 3 (la 4 no se incluye)\n",
        "print(\"\\nSlice de filas por posición [1:4]:\")\n",
        "print(slice_filas_iloc)\n",
        "\n",
        "# Seleccionar un valor específico (escalar) por posición de fila y posición de columna\n",
        "# Nombre_Producto está en posición 1, Precio_Unitario en posición 3\n",
        "valor_pos_1_3 = df.iloc[1, 3] # Fila en posición 1, columna en posición 3 (Precio_Unitario de Banana)\n",
        "print(f\"\\nValor en [fila_pos=1, col_pos=3]: {valor_pos_1_3}\")\n",
        "\n",
        "# Seleccionar todas las filas para columnas en posiciones específicas\n",
        "# ID_Producto (pos 0), Nombre_Producto (pos 1), En_Oferta (pos 6)\n",
        "columnas_pos_0_1_6 = df.iloc[:, [0, 1, 6]]\n",
        "print(\"\\nTodas las filas, columnas en posiciones 0, 1 y 6:\")\n",
        "print(columnas_pos_0_1_6)\n",
        "\n",
        "# Seleccionar un subconjunto de filas y columnas por posiciones\n",
        "# Filas en posiciones 0 y 1, y columnas en posiciones 1, 2, 3\n",
        "subconjunto_iloc = df.iloc[0:2, 1:4]\n",
        "print(\"\\nSubconjunto con iloc (filas 0-1, columnas 1-3):\")\n",
        "print(subconjunto_iloc)\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "id": "zhUzrdecbktG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Filtrado Condicional (Boolean Indexing)**\n",
        "  * Crear una condición booleana (devuelve una Serie de True/False):"
      ],
      "metadata": {
        "id": "CIZPuRTPbuP-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Condición: Productos con Precio_Unitario > 1.0\n",
        "condicion_precio = df['Precio_Unitario'] > 1.0\n",
        "\n",
        "print(\"Serie booleana para Precio_Unitario > 1.0:\")\n",
        "print(condicion_precio)\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "id": "IYzGsW77b5A3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "  * Usar la condición para filtrar el DataFrame:\n",
        "    * Pasas la Serie booleana dentro de los corchetes del DataFrame."
      ],
      "metadata": {
        "id": "MT5MKdBvb-Xo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "productos_caros = df[condicion_precio]\n",
        "# Alternativamente, de forma directa:\n",
        "# productos_caros = df[df['Precio_Unitario'] > 1.0]\n",
        "\n",
        "print(\"Productos con Precio_Unitario > 1.0:\")\n",
        "print(productos_caros)\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "id": "vGVpj9Mvb7qH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "  * Combinar múltiples condiciones:\n",
        "\n",
        "    * `&` para `AND` (ambas condiciones deben ser verdaderas)\n",
        "    * `|` para `OR` (al menos una condición debe ser verdadera)\n",
        "    * `~` para `NOT` (negar una condición) ¡Importante! Envuelve cada condición individual entre paréntesis () cuando combines."
      ],
      "metadata": {
        "id": "3TUcvMvzcHIB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Condición: Productos 'En_Oferta' Y con 'Stock_Disponible' < 100\n",
        "condicion_oferta_poco_stock = (df['En_Oferta'] == True) & (df['Stock_Disponible'] < 100)\n",
        "ofertas_limitadas = df[condicion_oferta_poco_stock]\n",
        "\n",
        "print(\"Productos en oferta Y con stock < 100:\")\n",
        "print(ofertas_limitadas)\n",
        "\n",
        "# Condición: Nombre_Producto es 'Manzana' O 'Naranja'\n",
        "condicion_manzana_o_naranja = (df['Nombre_Producto'] == 'Manzana') | (df['Nombre_Producto'] == 'Naranja')\n",
        "manzanas_o_naranjas = df[condicion_manzana_o_naranja]\n",
        "\n",
        "print(\"\\nProductos que son 'Manzana' O 'Naranja':\")\n",
        "print(manzanas_o_naranjas)\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "id": "AAMWxQTTcYdT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "  * Uso de `.isin()`:\n",
        "    * Para filtrar filas donde el valor de una columna esté presente en una lista de valores."
      ],
      "metadata": {
        "id": "tFO2yWyTcgZA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Productos que son 'Uva', 'Pera' o 'Kiwi'\n",
        "frutas_seleccionadas = ['Uva', 'Pera', 'Kiwi']\n",
        "df_frutas_seleccionadas = df[df['Nombre_Producto'].isin(frutas_seleccionadas)]\n",
        "print(\"Productos que son Uva, Pera o Kiwi:\")\n",
        "print(df_frutas_seleccionadas)\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "id": "SVXklqrdc0e1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "  * Uso de `.isnull()` / `.notnull():`\n",
        "    * Para encontrar o excluir filas con valores ausentes (NaN)."
      ],
      "metadata": {
        "id": "yB10gXw4c1fr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Productos con Precio_Unitario ausente (NaN)\n",
        "precio_ausente = df[df['Precio_Unitario'].isnull()]\n",
        "print(\"Productos con Precio_Unitario ausente:\")\n",
        "print(precio_ausente)\n",
        "\n",
        "# Productos con Stock_Disponible NO ausente\n",
        "stock_presente = df[df['Stock_Disponible'].notnull()]\n",
        "print(\"\\nProductos con Stock_Disponible presente:\")\n",
        "print(stock_presente)\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "id": "hP68__T-eRZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Manipulación de Datos**\n",
        "Es donde transformamos y damos forma a nuestros datos para prepararlos para el análisis, la visualización o el modelado. Aquí es donde Pandas realmente brilla.\n",
        "\n",
        "Para este ejercicio seguiremos con nuestro dataset de ejemplo:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Recreamos nuestro DataFrame de ejemplo\n",
        "datos_ejemplo = {\n",
        "    'ID_Producto': [101, 102, 103, 104, 105, 106, 107, 108],\n",
        "    'Nombre_Producto': ['Manzana', 'Banana', 'Naranja', 'Manzana', 'Uva', 'Pera', 'Banana', 'Kiwi'],\n",
        "    'Categoría': ['Fruta', 'Fruta', 'Fruta', 'Fruta', 'Fruta', 'Fruta', 'Fruta', 'Fruta'],\n",
        "    'Precio_Unitario': [1.2, 0.5, 0.8, 1.2, 2.5, 1.5, 0.6, np.nan],\n",
        "    'Stock_Disponible': [100, 150, np.nan, 80, 60, 120, 140, 75],\n",
        "    'Fecha_Ingreso': pd.to_datetime(['2024-01-10', '2024-01-11', '2024-01-10', '2024-01-12',\n",
        "                                   '2024-01-13', '2024-01-11', '2024-01-14', '2024-01-15']),\n",
        "    'En_Oferta': [False, True, False, False, True, True, False, True]\n",
        "}\n",
        "df = pd.DataFrame(datos_ejemplo)\n",
        "\n",
        "print(\"DataFrame original para Manipulación de datos:\")\n",
        "print(df)\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "aQnz3TvZeXDv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"DataFrame original para Manipulación de datos:\")\n",
        "print(df)\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "id": "-8mUzt6lf2tF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Para los ejemplos de manipulación, a veces es útil trabajar con una copia\n",
        "df_manip = df.copy()"
      ],
      "metadata": {
        "id": "pUMGqZnkf0_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Añadir Nuevas Columnas**\n",
        "\n",
        "  * Asignando un valor escalar (el mismo valor para todas las filas):"
      ],
      "metadata": {
        "id": "m28ii7c5f44i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_manip['País_Origen'] = 'Colombia'\n",
        "print(\"DataFrame con nueva columna 'País_Origen':\")\n",
        "print(df_manip.head())\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "id": "xFXGU7-VgJGd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "  * Creando una columna a partir de columnas existentes:"
      ],
      "metadata": {
        "id": "d4QGhjVfgLPC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcular el valor total del stock (Precio * Stock)\n",
        "# Primero, vamos a rellenar los NaN en Precio y Stock para este cálculo (veremos fillna() más adelante)\n",
        "df_manip['Precio_Unitario_NoNaN'] = df_manip['Precio_Unitario'].fillna(0) # Rellenamos NaN con 0 temporalmente\n",
        "df_manip['Stock_Disponible_NoNaN'] = df_manip['Stock_Disponible'].fillna(0) # Rellenamos NaN con 0 temporalmente\n",
        "\n",
        "df_manip['Valor_Total_Stock'] = df_manip['Precio_Unitario_NoNaN'] * df_manip['Stock_Disponible_NoNaN']\n",
        "print(\"DataFrame con columna 'Valor_Total_Stock':\")\n",
        "print(df_manip[['Nombre_Producto', 'Precio_Unitario_NoNaN', 'Stock_Disponible_NoNaN', 'Valor_Total_Stock']].head())\n",
        "\n",
        "# Eliminamos las columnas temporales que creamos para el cálculo\n",
        "df_manip = df_manip.drop(columns=['Precio_Unitario_NoNaN', 'Stock_Disponible_NoNaN'])\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "id": "NgGy3sIZgK1I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "  * Creando una columna basada en una condición (usando `np.where` o una función con `apply`):"
      ],
      "metadata": {
        "id": "KEELGzVAgR3V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Marcar si el producto es \"Premium\" si su precio es > 1.5\n",
        "df_manip['Es_Premium'] = np.where(df_manip['Precio_Unitario'] > 1.5, 'Sí', 'No')\n",
        "# Si Precio_Unitario es NaN, también será 'No' porque la condición será False o generará error si no se maneja\n",
        "# Para ser más robustos con NaN:\n",
        "df_manip['Es_Premium_v2'] = 'No' # Valor por defecto\n",
        "df_manip.loc[df_manip['Precio_Unitario'] > 1.5, 'Es_Premium_v2'] = 'Sí'\n",
        "df_manip.loc[df_manip['Precio_Unitario'].isnull(), 'Es_Premium_v2'] = 'Desconocido' # Manejo explícito de NaN\n",
        "\n",
        "print(\"DataFrame con columna 'Es_Premium_v2':\")\n",
        "print(df_manip[['Nombre_Producto', 'Precio_Unitario', 'Es_Premium_v2']].head())\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "id": "mWTXHuYhgZbd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Modificar Columnas**\n",
        "\n",
        "  * Reasignando todos los valores de una columna:"
      ],
      "metadata": {
        "id": "xlaaPs19gfdb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Supongamos que la categoría de todos los productos cambia\n",
        "# df_manip['Categoría'] = 'Fruta Tropical' # Esto cambiaría todas las filas\n",
        "# print(\"DataFrame con 'Categoría' modificada:\")\n",
        "# print(df_manip.head())\n",
        "# print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "# Comentado para no afectar demasiado los siguientes ejemplos."
      ],
      "metadata": {
        "id": "SdHL8i9PgpIA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "  * Usando `Series.apply()` o `Series.map()` para transformaciones elemento a elemento:\n",
        "\n",
        "    * `apply()` es más general, puede usarse con funciones que devuelven un escalar o una Serie.\n",
        "    * `map()` es útil para sustituir valores basados en un diccionario o aplicar una función simple."
      ],
      "metadata": {
        "id": "4jmvq6igg1d7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convertir Nombre_Producto a mayúsculas usando apply con una función lambda\n",
        "df_manip['Nombre_Producto_Mayus'] = df_manip['Nombre_Producto'].apply(lambda x: x.upper())\n",
        "\n",
        "# Crear una columna 'IVA_Producto' aplicando un 19% al precio unitario\n",
        "def calcular_iva(precio):\n",
        "    if pd.isna(precio):\n",
        "        return np.nan\n",
        "    return precio * 0.19\n",
        "\n",
        "df_manip['IVA_Producto'] = df_manip['Precio_Unitario'].apply(calcular_iva)\n",
        "\n",
        "print(\"DataFrame con 'Nombre_Producto_Mayus' e 'IVA_Producto':\")\n",
        "print(df_manip[['Nombre_Producto', 'Nombre_Producto_Mayus', 'Precio_Unitario', 'IVA_Producto']].head())\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "id": "N3dvdELqhCU1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "  * Usando `.replace()` para sustituir valores específicos:"
      ],
      "metadata": {
        "id": "cYNJf39chITW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Supongamos que queremos estandarizar 'Banana' a 'Banano'\n",
        "df_manip['Nombre_Producto'] = df_manip['Nombre_Producto'].replace('Banana', 'Banano')\n",
        "print(\"DataFrame con 'Banana' reemplazado por 'Banano':\")\n",
        "print(df_manip['Nombre_Producto'].value_counts())\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "id": "BLEidHUphcCK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Eliminar Columnas y Filas con drop()**\n",
        "\n",
        "  * `axis=0` para filas (por defecto).\n",
        "  * `axis=1` para columnas.\n",
        "  * `inplace=True` para modificar el DataFrame original (¡úsalo con cuidado!). Por defecto es False y devuelve una copia."
      ],
      "metadata": {
        "id": "Ntf_rGQnhevj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_copia_para_drop = df_manip.copy() # Trabajamos en una copia para no afectar df_manip\n",
        "\n",
        "# Eliminar una columna\n",
        "df_copia_para_drop = df_copia_para_drop.drop('País_Origen', axis=1)\n",
        "print(\"DataFrame después de eliminar 'País_Origen':\")\n",
        "print(df_copia_para_drop.head())\n",
        "\n",
        "# Eliminar múltiples columnas\n",
        "df_copia_para_drop = df_copia_para_drop.drop(columns=['Es_Premium', 'Es_Premium_v2']) # Otra forma de especificar columnas\n",
        "print(\"\\nDataFrame después de eliminar 'Es_Premium' y 'Es_Premium_v2':\")\n",
        "print(df_copia_para_drop.head())\n",
        "\n",
        "# Eliminar una fila por su etiqueta de índice (ej. la fila con índice 0)\n",
        "df_copia_para_drop = df_copia_para_drop.drop(0, axis=0) # o simplemente df_copia_para_drop.drop(0)\n",
        "print(\"\\nDataFrame después de eliminar la fila con índice 0:\")\n",
        "print(df_copia_para_drop.head())\n",
        "\n",
        "# Eliminar usando inplace (modifica df_copia_para_drop directamente)\n",
        "# df_copia_para_drop.drop('IVA_Producto', axis=1, inplace=True)\n",
        "# print(\"\\nDataFrame después de eliminar 'IVA_Producto' con inplace=True:\")\n",
        "# print(df_copia_para_drop.head())\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "id": "x7MCEainzTkz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Manejo de Valores Ausentes (NaN)\n",
        "\n",
        "  * Contar valores ausentes (repaso):"
      ],
      "metadata": {
        "id": "xSDKjG6ozgqq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Valores ausentes por columna en df_manip:\")\n",
        "print(df_manip.isnull().sum())\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "id": "098XAsHmzdb8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Eliminar filas/columnas con valores ausentes: `dropna()`:\n"
      ],
      "metadata": {
        "id": "Hr0sjberzlit"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_sin_na_filas = df_manip.dropna() # Elimina cualquier fila con al menos un NaN\n",
        "print(\"DataFrame después de df_manip.dropna() (elimina filas con NaN):\")\n",
        "print(df_sin_na_filas) # Observa que las filas con NaN en Precio o Stock desaparecieron\n",
        "\n",
        "df_sin_na_columnas = df_manip.dropna(axis=1) # Elimina cualquier columna con al menos un NaN\n",
        "print(\"\\nDataFrame después de df_manip.dropna(axis=1) (elimina columnas con NaN):\")\n",
        "print(df_sin_na_columnas.head()) # Observa que Precio y Stock desaparecieron\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "id": "TJ0olrCdz4p-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Rellenar valores ausentes: `fillna()`:"
      ],
      "metadata": {
        "id": "xZzAthY4z7i7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rellenar todos los NaN con un valor específico (ej. 0)\n",
        "df_relleno_0 = df_manip.fillna(0)\n",
        "print(\"DataFrame con NaN rellenados con 0:\")\n",
        "print(df_relleno_0[['Precio_Unitario', 'Stock_Disponible']].head())\n",
        "\n",
        "# Rellenar NaN en 'Precio_Unitario' con la media de esa columna\n",
        "media_precio = df_manip['Precio_Unitario'].mean()\n",
        "df_manip['Precio_Unitario_Relleno'] = df_manip['Precio_Unitario'].fillna(media_precio)\n",
        "print(\"\\nDataFrame con 'Precio_Unitario' NaN rellenado con la media:\")\n",
        "print(df_manip[['Nombre_Producto', 'Precio_Unitario', 'Precio_Unitario_Relleno']].head())\n",
        "\n",
        "# Rellenar usando el método 'ffill' (forward fill) - propaga el último valor válido hacia adelante\n",
        "df_manip['Stock_ffill'] = df_manip['Stock_Disponible'].fillna(method='ffill')\n",
        "print(\"\\nDataFrame con 'Stock_Disponible' NaN rellenado con ffill:\")\n",
        "print(df_manip[['Nombre_Producto', 'Stock_Disponible', 'Stock_ffill']])\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "id": "BMe9diIY0F3C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "  * Renombrar Columnas e Índices con `rename()`"
      ],
      "metadata": {
        "id": "SQmDr2aV0JS-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_renombrado = df_manip.rename(\n",
        "    columns={\n",
        "        'ID_Producto': 'ID',\n",
        "        'Nombre_Producto': 'Producto',\n",
        "        'Precio_Unitario_Relleno': 'Precio_Final'\n",
        "    },\n",
        "    index={0: 'Prod_A', 1: 'Prod_B'} # Renombra algunas etiquetas del índice\n",
        ")\n",
        "print(\"DataFrame con columnas e índices renombrados:\")\n",
        "print(df_renombrado.head())\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "id": "75008-eD0Pc3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Cambiar Tipos de Datos con `astype()`:"
      ],
      "metadata": {
        "id": "oZprCxiz0hQ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# df_manip aún tiene Stock_Disponible con NaN. Para convertir a int, primero rellenamos.\n",
        "df_manip['Stock_Disponible'] = df_manip['Stock_Disponible'].fillna(0)\n",
        "df_manip['Stock_Entero'] = df_manip['Stock_Disponible'].astype(int)\n",
        "\n",
        "# Convertir 'Categoría' a tipo 'category' (más eficiente para strings con pocas variantes)\n",
        "df_manip['Categoría_Tipo'] = df_manip['Categoría'].astype('category')\n",
        "\n",
        "print(\"Tipos de datos después de astype():\")\n",
        "print(df_manip[['Stock_Entero', 'Categoría_Tipo']].dtypes)\n",
        "print(df_manip.head())\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "id": "tkjFV9MR0kwV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Ordenar Datos**\n",
        "  * `sort_values()`: Ordenar por los valores de una o más columnas."
      ],
      "metadata": {
        "id": "18EYQ8lD0ofx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ordenar por 'Precio_Unitario' de forma descendente\n",
        "df_ordenado_precio = df_manip.sort_values(by='Precio_Unitario', ascending=False)\n",
        "print(\"DataFrame ordenado por Precio_Unitario (descendente):\")\n",
        "print(df_ordenado_precio[['Nombre_Producto', 'Precio_Unitario']].head())\n",
        "\n",
        "# Ordenar por 'Categoría' (alfabético) y luego por 'Stock_Disponible' (ascendente)\n",
        "df_ordenado_multi = df_manip.sort_values(by=['Categoría', 'Stock_Disponible'])\n",
        "print(\"\\nDataFrame ordenado por Categoría y luego por Stock_Disponible:\")\n",
        "print(df_ordenado_multi.head())\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "id": "xXwLGoDq09In"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`sort_index()`: Ordenar por las etiquetas del índice."
      ],
      "metadata": {
        "id": "MKEOTdQw1Ckz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_ordenado_indice = df_renombrado.sort_index(ascending=False) # Usamos el df con índices renombrados\n",
        "print(\"DataFrame (df_renombrado) ordenado por índice (descendente):\")\n",
        "print(df_ordenado_indice.head())\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "id": "7kI-f1kh1F-n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Resetear el Índice con reset_index()**\n",
        "Útil después de filtrados o reordenamientos que pueden dejar el índice desordenado o con huecos."
      ],
      "metadata": {
        "id": "glH9FKDb1OKb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_indice_reseteado = df_ordenado_precio.reset_index()                # El índice antiguo se convierte en una columna 'index'\n",
        "print(\"DataFrame con índice reseteado (el índice antiguo es ahora una columna):\")\n",
        "print(df_indice_reseteado.head())\n",
        "\n",
        "df_indice_reseteado_drop = df_ordenado_precio.reset_index(drop=True)  # El índice antiguo se descarta\n",
        "print(\"\\nDataFrame con índice reseteado (el índice antiguo se descarta):\")\n",
        "print(df_indice_reseteado_drop.head())\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "id": "IbX43wgS1Tm_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Agrupación de Datos (Group By)**\n",
        "Es una de las capacidades más potentes y flexibles de Pandas para el análisis de datos. Te permite realizar operaciones sobre subconjuntos de tus datos definidos por alguna característica común.\n",
        "\n",
        "Este proceso se conoce a menudo como \"Split-Apply-Combine\" (Dividir-Aplicar-Combinar):\n",
        "\n",
        "  * Split (Dividir): El DataFrame se divide en grupos basados en los valores de una o más columnas (las \"claves de agrupación\").\n",
        "  * Apply (Aplicar): Se aplica una función a cada grupo de forma independiente. Esta función puede ser una agregación (como calcular la suma o la media), una transformación (como estandarizar los datos dentro del grupo) o una filtración (como descartar grupos pequeños).\n",
        "  * Combine (Combinar): Los resultados de aplicar la función a cada grupo se combinan en una nueva estructura de datos (generalmente un DataFrame o una Serie)"
      ],
      "metadata": {
        "id": "6LxyTeAU1reB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Agrupación de Datos (Group By):\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\"\"\"\n",
        "  Hemos modificado un poco el DataFrame para tener más variedad en 'Categoría' y 'Nombre_Producto' y añadido 'Valoracion_Cliente' para tener más columnas numéricas.\n",
        "\"\"\"\n",
        "\n",
        "# Recreamos nuestro DataFrame de ejemplo\n",
        "datos_ejemplo = {\n",
        "    'ID_Producto': [101, 102, 103, 104, 105, 106, 107, 108, 109, 110],\n",
        "    'Nombre_Producto': ['Manzana', 'Banana', 'Naranja', 'Manzana', 'Uva', 'Pera', 'Banana', 'Kiwi', 'Manzana', 'Banana'],\n",
        "    'Categoría': ['Fruta', 'Fruta', 'Fruta', 'Fruta', 'Fruta', 'Fruta', 'Fruta', 'Fruta', 'Fruta Procesada', 'Fruta Procesada'],\n",
        "    'Precio_Unitario': [1.2, 0.5, 0.8, 1.2, 2.5, 1.5, 0.6, np.nan, 3.0, 2.2],\n",
        "    'Stock_Disponible': [100, 150, 80, 80, 60, 120, 140, 75, 50, 90], # Eliminamos NaN en Stock para simplificar agregaciones\n",
        "    'Valoracion_Cliente': [4.5, 4.0, 4.2, 4.5, 4.8, 3.9, 4.1, 4.6, 3.5, 4.3]\n",
        "}\n",
        "df = pd.DataFrame(datos_ejemplo)\n",
        "\n",
        "print(\"DataFrame original para el Paso 5:\")\n",
        "print(df)\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "id": "3XS4EZJu1o9W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "  * Usando `groupby()` Agrupar por una sola columna:\n",
        "    * El método `groupby()` crea un objeto DataFrameGroupBy. Por sí solo no muestra mucho, necesita que le apliques una función (como una agregación)."
      ],
      "metadata": {
        "id": "VBETY6uT2P7_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Agrupar por 'Nombre_Producto'\n",
        "agrupado_por_nombre = df.groupby('Nombre_Producto')\n",
        "\n",
        "print(\"Tipo de objeto después de groupby:\", type(agrupado_por_nombre))\n",
        "# <class 'pandas.core.groupby.generic.DataFrameGroupBy'>\n",
        "\n",
        "# Ahora aplicamos una función de agregación, por ejemplo, la media del Precio_Unitario para cada producto\n",
        "media_precio_por_producto = agrupado_por_nombre['Precio_Unitario'].mean()\n",
        "print(\"\\nMedia del Precio Unitario por Nombre_Producto:\")\n",
        "print(media_precio_por_producto)\n",
        "\n",
        "# Podemos calcular el stock total por producto\n",
        "stock_total_por_producto = agrupado_por_nombre['Stock_Disponible'].sum()\n",
        "print(\"\\nStock Total por Nombre_Producto:\")\n",
        "print(stock_total_por_producto)\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "id": "MZa-4OeV2aVZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "  * Agrupar por múltiples columnas:\n",
        "    * Puedes pasar una lista de nombres de columnas a `groupby()`. Esto crea un índice jerárquico (MultiIndex)."
      ],
      "metadata": {
        "id": "TAWgBzR_2btx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Agrupar por 'Categoría' y luego por 'Nombre_Producto'\n",
        "agrupado_multi = df.groupby(['Categoría', 'Nombre_Producto'])\n",
        "\n",
        "# Calcular la media de las columnas numéricas para estas agrupaciones\n",
        "media_multi_agrupacion = agrupado_multi.mean() # Calcula media para todas las columnas numéricas aplicables\n",
        "print(\"\\nMedia de columnas numéricas por Categoría y Nombre_Producto:\")\n",
        "print(media_multi_agrupacion[['Precio_Unitario', 'Stock_Disponible', 'Valoracion_Cliente']])\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "id": "5tGr0zcJ2mtG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Funciones de Agregación Comunes**\n",
        "\n",
        "Puedes aplicar directamente varias funciones de agregación a un objeto GroupBy:\n",
        "\n",
        "  * `count()`: Número de valores no nulos por grupo.\n",
        "  * `size()`: Número total de filas por grupo (incluye NaN).\n",
        "  * `sum()`: Suma de los valores.\n",
        "  * `mean()`: Media de los valores.\n",
        "  * `median()`: Mediana de los valores.\n",
        "  * `min()`: Valor mínimo.\n",
        "  * `max()`: Valor máximo.\n",
        "  * `std()`: Desviación estándar.\n",
        "  * `var()`: Varianza.\n",
        "  * `first()`: Primer valor no nulo.\n",
        "  * `last()`: Último valor no nulo.\n",
        "  * `nunique()`: Número de valores únicos."
      ],
      "metadata": {
        "id": "0Hj-6ldH2oOf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agrupado_categoria = df.groupby('Categoría')\n",
        "\n",
        "print(\"Conteo de productos (no nulos) por categoría (count()):\")\n",
        "print(agrupado_categoria.count()) # Muestra conteo para cada columna\n",
        "\n",
        "print(\"\\nTamaño de cada categoría (size()):\")\n",
        "print(agrupado_categoria.size()) # Devuelve una Serie con el tamaño de cada grupo\n",
        "\n",
        "print(\"\\nPrecio Unitario Mínimo y Máximo por Categoría:\")\n",
        "print(agrupado_categoria['Precio_Unitario'].min())\n",
        "print(agrupado_categoria['Precio_Unitario'].max())\n",
        "\n",
        "print(\"\\nMedia de Stock y Valoración por Categoría:\")\n",
        "print(agrupado_categoria[['Stock_Disponible', 'Valoracion_Cliente']].mean())\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "id": "stVMOblQ3Sma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Usando el método `agg()` para Múltiples Agregaciones\n",
        "\n",
        "El método `agg()` (o su alias `aggregate()`) es muy flexible:\n",
        "\n",
        "  * Aplicar una lista de funciones de agregación:"
      ],
      "metadata": {
        "id": "htLUiU-Z3Xjo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Aplicar suma, media y conteo al Precio_Unitario por Categoría\n",
        "agregaciones_precio = agrupado_categoria['Precio_Unitario'].agg(['sum', 'mean', 'count', 'std'])\n",
        "print(\"Múltiples agregaciones para Precio_Unitario por Categoría:\")\n",
        "print(agregaciones_precio)\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "id": "T8fW08lO-aLa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "  * Aplicar diferentes funciones a diferentes columnas:\n",
        "    * Se pasa un diccionario donde las claves son los nombres de las columnas y los valores son las funciones (o listas de funciones) a aplicar."
      ],
      "metadata": {
        "id": "nWTlFQIz-bQ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agregaciones_especificas = df.groupby('Categoría').agg(\n",
        "    precio_medio=('Precio_Unitario', 'mean'),             # Nueva forma de nombrar columnas resultado\n",
        "    stock_total=('Stock_Disponible', 'sum'),\n",
        "    valoracion_min=('Valoracion_Cliente', 'min'),\n",
        "    valoracion_max=('Valoracion_Cliente', 'max'),\n",
        "    numero_productos=('ID_Producto', 'count')\n",
        ")\n",
        "print(\"Agregaciones específicas por Categoría con nombres de columna personalizados:\")\n",
        "print(agregaciones_especificas)\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "# Forma más antigua (también funciona):\n",
        "# agregaciones_especificas_v_antigua = df.groupby('Categoría').agg(\n",
        "#     {'Precio_Unitario': 'mean',\n",
        "#      'Stock_Disponible': 'sum',\n",
        "#      'Valoracion_Cliente': ['min', 'max'],\n",
        "#      'ID_Producto': 'count'}\n",
        "# )\n",
        "# print(\"Agregaciones específicas por Categoría (forma antigua):\")\n",
        "# print(agregaciones_especificas_v_antigua)"
      ],
      "metadata": {
        "id": "GWHK8y9l-tFE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "  * Aplicar funciones personalizadas (lambda o definidas):"
      ],
      "metadata": {
        "id": "XQiBWCg_-uPG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rango_precios(serie_precios):\n",
        "    # Ignorar NaNs para min/max si existen, o devolver NaN si todo es NaN\n",
        "    if serie_precios.isnull().all():\n",
        "        return np.nan\n",
        "    return serie_precios.max() - serie_precios.min()\n",
        "\n",
        "agregaciones_custom = df.groupby('Categoría').agg(\n",
        "    precio_medio=('Precio_Unitario', 'mean'),\n",
        "    rango_de_precios=('Precio_Unitario', rango_precios), # Usando función definida\n",
        "    variedad_productos=('Nombre_Producto', lambda x: x.nunique()) # Usando lambda\n",
        ")\n",
        "print(\"Agregaciones con funciones personalizadas por Categoría:\")\n",
        "print(agregaciones_custom)\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "id": "5IHlxK3q-065"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Iterar sobre Grupos**\n",
        "\n",
        "  * Puedes iterar sobre un objeto GroupBy si necesitas hacer algo más complejo con cada grupo. Cada iteración te da el nombre (o tupla de nombres si agrupaste por múltiples columnas) del grupo y el sub-DataFrame correspondiente a ese grupo."
      ],
      "metadata": {
        "id": "MDasvAi9-2LP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Iterando sobre los grupos de 'Nombre_Producto':\")\n",
        "for nombre_prod, grupo_df in df.groupby('Nombre_Producto'):\n",
        "    print(f\"\\n--- Grupo: {nombre_prod} ---\")\n",
        "    print(f\"Número de items en este grupo: {len(grupo_df)}\")\n",
        "    print(grupo_df[['Categoría', 'Precio_Unitario', 'Stock_Disponible']].head(2)) # Mostrar solo algunas columnas y filas\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "id": "9k9XORQX_AoJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Seleccionar un Grupo Específico con `get_group()`**\n",
        "\n",
        "* La agrupación es una herramienta analítica muy poderosa. Te permite segmentar tus datos y realizar cálculos comparativos o resumidos sobre esos segmentos de manera eficiente."
      ],
      "metadata": {
        "id": "yW3LBQ6F_Bt0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agrupado_categoria = df.groupby('Categoría')\n",
        "grupo_frutas = agrupado_categoria.get_group('Fruta')\n",
        "print(\"DataFrame solo para la categoría 'Fruta':\")\n",
        "print(grupo_frutas)\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "id": "rjLxqZO0_OuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Combinar y fusionar diferentes DataFrames**\n",
        "\n",
        "En el mundo real, es muy común que los datos que necesitas para un análisis estén distribuidos en múltiples archivos o tablas. Pandas nos ofrece herramientas muy poderosas para combinar estos DataFrames de diversas maneras, similar a lo que harías con JOINs en SQL.\n",
        "\n",
        "Las principales formas de combinar DataFrames en Pandas son:\n",
        "\n",
        "* **Concatenación (`pd.concat()`):** \"Pegar\" DataFrames uno debajo del otro (apilar filas) o uno al lado del otro (añadir columnas).\n",
        "* **Fusión tipo SQL (`pd.merge()`):** Combinar DataFrames basándose en los valores de una o más columnas comunes (claves), similar a los JOIN de SQL.\n",
        "* **Unión por Índice (`.join()`):** Un método de instancia que permite unir DataFrames principalmente por sus índices.\n",
        "\n",
        "## **Concatenación con `pd.concat()`**\n",
        "\n",
        "La concatenación es útil cuando tienes DataFrames con la misma estructura (o similar) y quieres unirlos."
      ],
      "metadata": {
        "id": "b80cYBf0_j8_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Creando DataFrames de ejemplo: \"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "df1 = pd.DataFrame({\n",
        "    'A': ['A0', 'A1', 'A2', 'A3'],\n",
        "    'B': ['B0', 'B1', 'B2', 'B3'],\n",
        "    'C': ['C0', 'C1', 'C2', 'C3'],\n",
        "    'D': ['D0', 'D1', 'D2', 'D3']\n",
        "}, index=[0, 1, 2, 3])\n",
        "\n",
        "df2 = pd.DataFrame({\n",
        "    'A': ['A4', 'A5', 'A6', 'A7'],\n",
        "    'B': ['B4', 'B5', 'B6', 'B7'],\n",
        "    'C': ['C4', 'C5', 'C6', 'C7'],\n",
        "    'D': ['D4', 'D5', 'D6', 'D7']\n",
        "}, index=[4, 5, 6, 7]) # Índices diferentes para mostrar cómo se manejan\n",
        "\n",
        "df3 = pd.DataFrame({\n",
        "    'A': ['A8', 'A9', 'A10', 'A11'],\n",
        "    'B': ['B8', 'B9', 'B10', 'B11'],\n",
        "    'C': ['C8', 'C9', 'C10', 'C11'],\n",
        "    'D': ['D8', 'D9', 'D10', 'D11']\n",
        "}, index=[0, 1, 2, 3]) # Índices que se solapan con df1\n",
        "\n",
        "print(\"df1:\")\n",
        "print(df1)\n",
        "print(\"\\ndf2:\")\n",
        "print(df2)\n",
        "print(\"\\ndf3 (índices duplicados con df1):\")\n",
        "print(df3)\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "id": "uEVUn4oi_0HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Concatenación por Filas (axis=0, comportamiento por defecto):**\n",
        "Apila los DataFrames uno debajo del otro."
      ],
      "metadata": {
        "id": "-qaEczU9uCvU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Concatenar df1 y df2\n",
        "concatenado_filas = pd.concat([df1, df2])\n",
        "print(\"Concatenación de df1 y df2 por filas (índices originales):\")\n",
        "print(concatenado_filas)\n",
        "\n",
        "# Concatenar df1 y df3 (índices duplicados)\n",
        "concatenado_indices_duplicados = pd.concat([df1, df3])\n",
        "print(\"\\nConcatenación de df1 y df3 (índices duplicados):\")\n",
        "print(concatenado_indices_duplicados)\n",
        "# Si intentas seleccionar por índice duplicado, ej. .loc[0], obtendrás múltiples filas\n",
        "\n",
        "# Para evitar índices duplicados, puedes resetear el índice:\n",
        "concatenado_reset_index = pd.concat([df1, df3], ignore_index=True)\n",
        "print(\"\\nConcatenación de df1 y df3 con ignore_index=True:\")\n",
        "print(concatenado_reset_index)\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "id": "bRxM87J0t_0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Concatenación por Columnas `(axis=1)`:**\n",
        "\"Pega\" los DataFrames uno al lado del otro. La alineación se basa en el índice. Si los índices no coinciden perfectamente, se introducirán valores NaN."
      ],
      "metadata": {
        "id": "OfZOLoHZuO2w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df4 = pd.DataFrame({\n",
        "    'E': ['E0', 'E1', 'E2', 'E3'],\n",
        "    'F': ['F0', 'F1', 'F2', 'F3']\n",
        "}, index=[0, 1, 2, 3])\n",
        "\n",
        "df5 = pd.DataFrame({\n",
        "    'G': ['G0', 'G1', 'G2', 'G3'],\n",
        "    'H': ['H0', 'H1', 'H2', 'H3']\n",
        "}, index=[2, 3, 4, 5]) # Índices parcialmente solapados y parcialmente nuevos\n",
        "\n",
        "concatenado_columnas = pd.concat([df1, df4], axis=1)\n",
        "print(\"Concatenación de df1 y df4 por columnas (mismos índices):\")\n",
        "print(concatenado_columnas)\n",
        "\n",
        "concatenado_columnas_indices_dif = pd.concat([df1, df5], axis=1)\n",
        "print(\"\\nConcatenación de df1 y df5 por columnas (índices diferentes):\")\n",
        "print(concatenado_columnas_indices_dif) # Observa los NaN\n",
        "\n",
        "# Para mantener solo las filas donde los índices coinciden en ambos DataFrames (join='inner')\n",
        "concatenado_columnas_inner = pd.concat([df1, df5], axis=1, join='inner')\n",
        "print(\"\\nConcatenación de df1 y df5 por columnas con join='inner':\")\n",
        "print(concatenado_columnas_inner)\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "id": "oSEjNhmUuWUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## **Fusión tipo SQL con `pd.merge()`**\n",
        "\n",
        "`pd.merge()` es la función principal para combinar DataFrames de forma similar a los JOIN en bases de datos relacionales."
      ],
      "metadata": {
        "id": "mGtIrZtEublL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Creamos dataframes para el ejercicio: \"\"\"\n",
        "df_clientes = pd.DataFrame({\n",
        "    'ID_Cliente': [1, 2, 3, 4, 5],\n",
        "    'Nombre': ['Ana', 'Luis', 'Marta', 'Juan', 'Sofia'],\n",
        "    'Ciudad': ['Madrid', 'Barcelona', 'Valencia', 'Sevilla', 'Madrid']\n",
        "})\n",
        "\n",
        "df_pedidos = pd.DataFrame({\n",
        "    'ID_Pedido': [101, 102, 103, 104, 105, 106],\n",
        "    'ID_Cliente': [1, 2, 1, 3, 5, 7], # Cliente 7 no está en df_clientes\n",
        "    'Producto': ['Laptop', 'Mouse', 'Teclado', 'Monitor', 'Webcam', 'Impresora'],\n",
        "    'Cantidad': [1, 2, 1, 1, 1, 1]\n",
        "})\n",
        "\n",
        "df_productos = pd.DataFrame({\n",
        "    'Nombre_Prod': ['Laptop', 'Mouse', 'Teclado', 'Monitor', 'Webcam', 'SSD'],\n",
        "    'Precio': [1200, 25, 75, 300, 50, 150]\n",
        "})\n",
        "\n",
        "print(\"df_clientes:\")\n",
        "print(df_clientes)\n",
        "print(\"\\ndf_pedidos:\")\n",
        "print(df_pedidos)\n",
        "print(\"\\ndf_productos:\")\n",
        "print(df_productos)\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "id": "oNvVOUJMuu5n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Merge con una clave común (on):**\n",
        "Por defecto, `pd.merge()` realiza un inner join."
      ],
      "metadata": {
        "id": "AIG-zfsLu0eP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Unir clientes con sus pedidos\n",
        "df_clientes_pedidos = pd.merge(df_clientes, df_pedidos, on='ID_Cliente')\n",
        "print(\"Merge (inner join por defecto) de clientes y pedidos en 'ID_Cliente':\")\n",
        "print(df_clientes_pedidos)\n",
        "# Nota: El cliente 4 (Juan) no aparece porque no tiene pedidos. El pedido del cliente 7 no aparece porque el cliente no está en df_clientes.\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "id": "JmAGoLfwvBLe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Tipos de Merge `(how)`:**\n",
        "\n",
        "* **`how='inner'` (por defecto):** Solo filas donde la clave de unión existe en ambos DataFrames.\n",
        "* **`how='outer'`:** Todas las filas de ambos DataFrames. Si no hay coincidencia, se rellena con NaN.\n",
        "* **`how='left'`:** Todas las filas del DataFrame izquierdo y las coincidentes del derecho. Si no hay coincidencia en el derecho, se rellena con NaN.\n",
        "* **`how='right'`:** Todas las filas del DataFrame derecho y las coincidentes del izquierdo. Si no hay coincidencia en el izquierdo, se rellena con NaN."
      ],
      "metadata": {
        "id": "s0ZGM4rJvO16"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Left Join: Todos los clientes, y sus pedidos si los tienen\n",
        "df_left_join = pd.merge(df_clientes, df_pedidos, on='ID_Cliente', how='left')\n",
        "print(\"Left Join de clientes y pedidos:\")\n",
        "print(df_left_join) # Juan (cliente 4) aparece con NaN en las columnas de pedido.\n",
        "\n",
        "# Right Join: Todos los pedidos, y la información del cliente si existe\n",
        "df_right_join = pd.merge(df_clientes, df_pedidos, on='ID_Cliente', how='right')\n",
        "print(\"\\nRight Join de clientes y pedidos:\")\n",
        "print(df_right_join) # El pedido del cliente 7  aparece con NaN en la info del cliente.\n",
        "\n",
        "# Outer Join: Todos los clientes y todos los pedidos\n",
        "df_outer_join = pd.merge(df_clientes, df_pedidos, on='ID_Cliente', how='outer')\n",
        "print(\"\\nOuter Join de clientes y pedidos:\")\n",
        "print(df_outer_join)\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "id": "WQqMDd5vvoJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Merge con claves con nombres diferentes (left_on, right_on):**"
      ],
      "metadata": {
        "id": "Z38pE4ULzp-8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Merge con claves con nombres diferentes (left_on, right_on):\n",
        "# Unir df_pedidos con df_productos.\n",
        "# La columna de producto en df_pedidos es 'Producto' y en df_productos es 'Nombre_Prod'\n",
        "df_pedidos_con_precio = pd.merge(df_pedidos, df_productos,\n",
        "                                 left_on='Producto', right_on='Nombre_Prod',\n",
        "                                 how='left') # Queremos todos los pedidos, y el precio si el producto existe\n",
        "print(\"Merge de pedidos y productos con claves de nombres diferentes:\")\n",
        "print(df_pedidos_con_precio)\n",
        "# Nota: Si hay un producto en df_pedidos que no está en df_productos, su precio será NaN.\n",
        "# La columna 'Nombre_Prod' (de df_productos) se incluye. Podemos eliminarla si no la necesitamos.\n",
        "# df_pedidos_con_precio = df_pedidos_con_precio.drop('Nombre_Prod', axis=1)\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "id": "gtwiaXUBzmED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Merge con múltiples claves:**\n",
        "Si necesitas unir por más de una columna, pasa una lista a `on.`\n",
        "\n",
        "```\n",
        "# Ejemplo conceptual (necesitaríamos DFs adecuados)\n",
        "# df_merged_multi = pd.merge(dfA, dfB, on=['Clave1', 'Clave2'])\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "9hbl8nFWzzBT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Manejo de columnas con nombres duplicados (no claves) con suffixes:**\n",
        "Si los DataFrames que unes tienen columnas (que no son las claves de unión) con el mismo nombre, merge añadirá sufijos `_x` y `_y` por defecto. Puedes personalizarlos."
      ],
      "metadata": {
        "id": "vXTfP6abz_AU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_info_adicional_cliente = pd.DataFrame({\n",
        "    'ID_Cliente': [1, 2, 6],\n",
        "    'Ciudad': ['Barcelona', 'Valencia', 'Bilbao'], # 'Ciudad' también existe en df_clientes\n",
        "    'Telefono': ['555-111', '555-222', '555-333']\n",
        "})\n",
        "\n",
        "df_merged_sufijos = pd.merge(df_clientes, df_info_adicional_cliente, on='ID_Cliente', how='left',\n",
        "                             suffixes=('_Cliente', '_Adicional'))\n",
        "print(\"Merge con manejo de sufijos para columnas duplicadas:\")\n",
        "print(df_merged_sufijos)\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "id": "KMee5GhJz3Ls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## **Unión con el método `.join()`**\n",
        "\n",
        "El método `.join()` de un DataFrame es una forma conveniente de combinar columnas de otro DataFrame. Por defecto, une por los índices.\n",
        "\n",
        "  * Creando DataFrames de ejemplo para .join():"
      ],
      "metadata": {
        "id": "w1f1gZTiz6qi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_izq = pd.DataFrame({'A': ['A0', 'A1', 'A2'],\n",
        "                       'B': ['B0', 'B1', 'B2']},\n",
        "                      index=['K0', 'K1', 'K2'])\n",
        "\n",
        "df_der = pd.DataFrame({'C': ['C0', 'C1', 'C2'],\n",
        "                       'D': ['D0', 'D1', 'D2']},\n",
        "                      index=['K0', 'K2', 'K3']) # Índices parcialmente coincidentes\n",
        "\n",
        "df_der_col_clave = pd.DataFrame({'Clave': ['K0', 'K1', 'K2'],\n",
        "                                 'E': ['E0', 'E1', 'E2']})\n",
        "\n",
        "print(\"df_izq:\")\n",
        "print(df_izq)\n",
        "print(\"\\ndf_der:\")\n",
        "print(df_der)\n",
        "print(\"\\ndf_der_col_clave:\")\n",
        "print(df_der_col_clave)\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "id": "1Dc7H6kH0wpk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Unir por índice (comportamiento por defecto, es un left join):**"
      ],
      "metadata": {
        "id": "PQ_yOYKp0zy7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "join_por_indice = df_izq.join(df_der) # Es un left join por defecto\n",
        "print(\"Join de df_izq y df_der por índice (left join):\")\n",
        "print(join_por_indice)\n",
        "\n",
        "join_por_indice_outer = df_izq.join(df_der, how='outer')\n",
        "print(\"\\nOuter Join de df_izq y df_der por índice:\")\n",
        "print(join_por_indice_outer)\n",
        "\n",
        "# Si hay columnas con el mismo nombre, necesitas sufijos\n",
        "df_izq_duplicado = df_izq.rename(columns={'B':'X'})\n",
        "df_der_duplicado = df_der.rename(columns={'C':'X'})\n",
        "join_sufijos = df_izq_duplicado.join(df_der_duplicado, lsuffix='_izq', rsuffix='_der', how='inner')\n",
        "print(\"\\nInner Join con sufijos para columnas 'X' duplicadas:\")\n",
        "print(join_sufijos)\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "id": "7_gIriRi01g7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Unir una columna del DataFrame izquierdo con el índice del DataFrame derecho:**\n",
        "Para esto, el DataFrame derecho primero debe tener su columna clave como índice usando `set_index().`"
      ],
      "metadata": {
        "id": "3WcO89xD1WYP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# df_izq tiene un índice K0, K1, K2\n",
        "# df_der_col_clave tiene una columna 'Clave' con K0, K1, K2 y otra columna 'E'\n",
        "# Queremos añadir la columna 'E' a df_izq donde el índice de df_izq coincida con 'Clave' de df_der_col_clave\n",
        "\n",
        "# Opción 1: Establecer 'Clave' como índice en el DF derecho y luego unir\n",
        "join_col_a_indice = df_izq.join(df_der_col_clave.set_index('Clave'))\n",
        "print(\"Join de df_izq con el índice de df_der_col_clave (creado desde su columna 'Clave'):\")\n",
        "print(join_col_a_indice)\n",
        "\n",
        "# Opción 2: El método 'on' en join se refiere a la columna del DataFrame IZQUIERDO\n",
        "# mientras que el DataFrame DERECHO se une usando su índice.\n",
        "# Esto es un poco menos común o más confuso que usar merge para uniones basadas en columnas.\n",
        "\n",
        "df_izq_con_clave = pd.DataFrame({'A': ['A0', 'A1', 'A2'],\n",
        "                               'B': ['B0', 'B1', 'B2'],\n",
        "                               'Clave_Union': ['K0', 'K1', 'K0']}, # Columna para unir\n",
        "                              index=['idx0','idx1','idx2'])\n",
        "\n",
        "df_der_con_indice = pd.DataFrame({'E': ['E_K0', 'E_K1', 'E_K2', 'E_K3']},\n",
        "                                index=['K0', 'K1', 'K2', 'K3']) # Índice para unir\n",
        "\n",
        "join_col_on = df_izq_con_clave.join(df_der_con_indice, on='Clave_Union')\n",
        "print(\"\\nJoin usando 'on' para columna de df_izq_con_clave y el índice de df_der_con_indice:\")\n",
        "print(join_col_on)\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "id": "KywY360j0550"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Generalmente, para uniones basadas en columnas, pd.merge() es más explícito y flexible. `.join()` brilla cuando la unión se basa principalmente en los índices.*"
      ],
      "metadata": {
        "id": "d7Z-l3kP1g-r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# **Entrada y Salida de Datos (I/O).**\n",
        "\n",
        "Una de las tareas más fundamentales en cualquier proyecto de análisis de datos es la capacidad de leer datos desde diversas fuentes y, una vez procesados, guardarlos o exportarlos. Pandas es excepcionalmente bueno en esto, ofreciendo funciones para una amplia variedad de formatos de archivo.\n",
        "\n",
        "## **Leer Datos desde Archivos**\n",
        "\n",
        "Las funciones de lectura en Pandas generalmente comienzan con `pd.read_*.`\n",
        "\n",
        "* Leer Archivos CSV (Valores Separados por Comas) con `pd.read_csv()`\n",
        "    * Los archivos CSV son uno de los formatos más comunes para almacenar datos tabulares. Son archivos de texto plano donde los valores suelen estar separados por comas (aunque se pueden usar otros delimitadores).\n",
        "\n",
        "*Como no podemos cargar archivos locales directamente en este entorno, vamos a simular el contenido de un archivo CSV usando una cadena de texto y luego leerlo con Pandas.*"
      ],
      "metadata": {
        "id": "javpC78H1vw1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import io # Necesario para simular archivos en memoria\n",
        "\n",
        "# 1. Simular el contenido de un archivo CSV\n",
        "csv_data_string = \"\"\"ID_Sensor,Fecha,Temperatura,Humedad,Ciudad\n",
        "S001,2025-05-01,25.5,60,Medellín\n",
        "S002,2025-05-01,22.1,55,Bogotá\n",
        "S003,2025-05-01,30.2,70,Cartagena\n",
        "S001,2025-05-02,26.0,62,Medellín\n",
        "S002,2025-05-02,N/A,58,Bogotá\n",
        "S003,2025-05-02,31.0,72,Cartagena\n",
        "\"\"\"\n",
        "# En un caso real, usarías: df_csv = pd.read_csv(\"ruta/a/tu/archivo.csv\")\n",
        "\n",
        "# Usamos io.StringIO para que Pandas lea la cadena como si fuera un archivo\n",
        "df_csv = pd.read_csv(io.StringIO(csv_data_string))\n",
        "print(\"DataFrame leído desde la cadena CSV simulada:\")\n",
        "print(df_csv)\n",
        "print(\"\\nTipos de datos iniciales:\")\n",
        "print(df_csv.dtypes) # Observa que Fecha es object y Temperatura/Humedad pueden ser object si hay no numéricos\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "# Parámetros comunes de pd.read_csv():\n",
        "# sep (o delimiter): Especifica el delimitador. Por defecto es ','.\n",
        "# Si tuvieras un CSV separado por punto y coma:\n",
        "csv_data_semicolon = \"Nombre;Edad;País\\nAna;30;Colombia\\nLuis;25;México\"\n",
        "df_semicolon = pd.read_csv(io.StringIO(csv_data_semicolon), sep=';')\n",
        "print(\"DataFrame leído desde CSV con punto y coma como separador:\")\n",
        "print(df_semicolon)\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "# header: Fila a usar como encabezado (nombres de columna). header=0 es por defecto.\n",
        "# names: Lista de nombres de columna a usar, útil si el archivo no tiene encabezado (usar header=None).\n",
        "csv_no_header = \"val1,val2,val3\\n10,20,30\\n40,50,60\"\n",
        "df_no_header = pd.read_csv(io.StringIO(csv_no_header), header=None, names=['ColA', 'ColB', 'ColC'])\n",
        "print(\"DataFrame leído desde CSV sin encabezado, con nombres de columna provistos:\")\n",
        "print(df_no_header)\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "# index_col: Columna a usar como índice del DataFrame.\n",
        "df_csv_con_indice = pd.read_csv(io.StringIO(csv_data_string), index_col='ID_Sensor')\n",
        "print(\"DataFrame CSV con 'ID_Sensor' como índice:\")\n",
        "print(df_csv_con_indice.head())\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "# usecols: Lista de columnas que quieres leer (para ahorrar memoria si solo necesitas algunas).\n",
        "df_csv_columnas_selectas = pd.read_csv(io.StringIO(csv_data_string), usecols=['Fecha', 'Temperatura', 'Ciudad'])\n",
        "print(\"DataFrame CSV solo con columnas 'Fecha', 'Temperatura', 'Ciudad':\")\n",
        "print(df_csv_columnas_selectas)\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "# dtype: Especificar tipos de datos para columnas al leer.\n",
        "# na_values: Lista de strings adicionales que deben ser reconocidos como NaN (además de los estándar).\n",
        "# skiprows: Número de filas a saltar al inicio del archivo.\n",
        "# nrows: Número de filas a leer (útil para inspeccionar archivos muy grandes).\n",
        "df_csv_avanzado = pd.read_csv(\n",
        "    io.StringIO(csv_data_string),\n",
        "    dtype={'Humedad': 'float64'}, # Forzar Humedad a float\n",
        "    na_values=['N/A', 'no disponible'], # 'N/A' ya es reconocido, pero es un ejemplo\n",
        "    skiprows=[1], # Saltar la primera fila de datos (S001,2025-05-01...) después del encabezado\n",
        "    nrows=3 # Leer solo las siguientes 3 filas de datos\n",
        ")\n",
        "print(\"DataFrame CSV con opciones avanzadas (dtype, na_values, skiprows, nrows):\")\n",
        "print(df_csv_avanzado)\n",
        "print(df_csv_avanzado.dtypes)\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "id": "xbahXEf23KUz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Nota importante** *sobre parse_dates: Si tienes columnas de fecha en formato de texto, `pd.read_csv() `tiene un parámetro `parse_dates=['NombreColumnaFecha1', 'NombreColumnaFecha2']` que intenta convertir esas columnas a tipo datetime automáticamente durante la lectura, lo cual es muy conveniente.*\n",
        "\n",
        "## **Leer Archivos Excel con pd.read_excel()**\n",
        "\n",
        "Pandas puede leer archivos .xls y .xlsx. Para esto, a menudo necesita que tengas instaladas librerías adicionales como openpyxl (para .xlsx) o xlrd (para formatos .xls más antiguos). Si no las tienes, Pandas te lo indicará.\n",
        "\n",
        "`pip install openpyxl xlrd`\n",
        "\n",
        "Simularemos la lectura de un archivo Excel. Primero, crearemos un \"archivo\" Excel en memoria."
      ],
      "metadata": {
        "id": "kHCSE9UR3bk-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import io\n",
        "\n",
        "# Crear un DataFrame de ejemplo para \"guardarlo\" en un buffer de Excel\n",
        "df_para_excel = pd.DataFrame({\n",
        "    'Trimestre': ['T1_2025', 'T2_2025', 'T3_2025'],\n",
        "    'Ventas_ProductoA': [1500, 1700, 1600],\n",
        "    'Ventas_ProductoB': [800, 950, 900]\n",
        "})\n",
        "\n",
        "df_para_excel_hoja2 = pd.DataFrame({\n",
        "    'Mes': ['Ene', 'Feb', 'Mar'],\n",
        "    'Gastos': [500, 550, 520]\n",
        "})\n",
        "\n",
        "# \"Guardar\" en un buffer de bytes en formato Excel\n",
        "buffer_excel = io.BytesIO()\n",
        "with pd.ExcelWriter(buffer_excel, engine='openpyxl') as writer: # engine='xlsxwriter' es otra opción\n",
        "    df_para_excel.to_excel(writer, sheet_name='Ventas_Trimestrales', index=False)\n",
        "    df_para_excel_hoja2.to_excel(writer, sheet_name='Gastos_Mensuales', index=False)\n",
        "buffer_excel.seek(0)                            # Regresar el puntero al inicio del buffer para leerlo\n",
        "\n",
        "# En un caso real, usarías: df_excel = pd.read_excel(\"ruta/a/tu/archivo.xlsx\")\n",
        "\n",
        "# Leer la primera hoja por defecto (o por índice 0)\n",
        "df_excel_hoja1 = pd.read_excel(buffer_excel, sheet_name=0)\n",
        "print(\"DataFrame leído desde la primera hoja del Excel simulado:\")\n",
        "print(df_excel_hoja1)\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "# Leer una hoja específica por su nombre\n",
        "df_excel_gastos = pd.read_excel(buffer_excel, sheet_name='Gastos_Mensuales')\n",
        "print(\"DataFrame leído desde la hoja 'Gastos_Mensuales':\")\n",
        "print(df_excel_gastos)\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "# Leer todas las hojas en un diccionario de DataFrames\n",
        "# (necesitamos reabrir el buffer o resetearlo si no se hace bien la primera vez)\n",
        "buffer_excel.seek(0) # Asegurar que el buffer está al inicio\n",
        "diccionario_hojas_excel = pd.read_excel(buffer_excel, sheet_name=None)\n",
        "print(\"Diccionario con todas las hojas del Excel:\")\n",
        "for nombre_hoja, df_hoja in diccionario_hojas_excel.items():\n",
        "    print(f\"\\n--- Hoja: {nombre_hoja} ---\")\n",
        "    print(df_hoja)\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "# Otros parámetros útiles en read_excel:\n",
        "# -----------------------------------------------------\n",
        "# header:       Fila a usar como encabezado.\n",
        "# names:        Nombres para las columnas.\n",
        "# index_col:    Columna a usar como índice.\n",
        "# usecols:      Columnas específicas a leer.\n",
        "# dtype:        Para especificar tipos de datos."
      ],
      "metadata": {
        "id": "tdQy8x0b6Q-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Mención de Otros Formatos de Lectura Comunes:***\n",
        "\n",
        "* **JSON (pd.read_json()):** Para leer archivos en formato JavaScript Object Notation. Puede manejar diferentes orientaciones de JSON (records, columns, etc.).\n",
        "* **SQL (pd.read_sql(), pd.read_sql_query(), pd.read_sql_table())**: Para leer datos directamente desde bases de datos relacionales. Requiere una conexión de base de datos (ej. con sqlalchemy).\n",
        "* **HTML (pd.read_html()):** Intenta extraer tablas de páginas HTML. Devuelve una lista de DataFrames.\n",
        "* **Pickle (pd.read_pickle()):** Para leer archivos en el formato binario específico de Pandas (pickle). Es rápido pero no es portable entre diferentes versiones de Pandas o para otros lenguajes.\n",
        "\n",
        "¡Y muchos más! (HDF5, Parquet, Stata, SAS, etc.)\n",
        "\n",
        "---\n",
        "# **Escribir Datos a Archivos**\n",
        "\n",
        "Las funciones de escritura en Pandas generalmente comienzan con df.to_*.\n",
        "\n",
        "### **Escribir a Archivos CSV con `df.to_csv()`**"
      ],
      "metadata": {
        "id": "6SiI5yEu63Ie"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Usaremos el df_csv que leímos antes\n",
        "print(\"DataFrame que vamos a 'guardar' en CSV:\")\n",
        "print(df_csv)\n",
        "\n",
        "# Simular la escritura a un buffer (en un caso real, darías una ruta de archivo)\n",
        "buffer_escritura_csv = io.StringIO()\n",
        "df_csv.to_csv(buffer_escritura_csv)\n",
        "print(\"\\nContenido del CSV 'guardado' en el buffer (incluye índice por defecto):\")\n",
        "print(buffer_escritura_csv.getvalue())\n",
        "\n",
        "# Parámetros comunes de df.to_csv():\n",
        "# index=False: Muy común, para no escribir el índice del DataFrame como una columna en el CSV.\n",
        "buffer_escritura_csv_no_index = io.StringIO()\n",
        "df_csv.to_csv(buffer_escritura_csv_no_index, index=False)\n",
        "print(\"\\nContenido del CSV 'guardado' sin el índice:\")\n",
        "print(buffer_escritura_csv_no_index.getvalue())\n",
        "\n",
        "# sep: Para especificar el delimitador (por defecto es ',').\n",
        "# header=False: Para no escribir la fila de encabezado (nombres de columna).\n",
        "# columns: Para especificar un subconjunto de columnas a escribir.\n",
        "# mode='a': Para añadir a un archivo existente en lugar de sobrescribirlo (mode='w' es por defecto).\n",
        "# encoding: Para especificar la codificación del archivo (ej. 'utf-8').\n",
        "\n",
        "# Ejemplo: df_csv.to_csv(\"mi_archivo_salida.csv\", index=False, sep=';', encoding='utf-8')\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "id": "CPLyOGZV6Uz3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Escribir a Archivos Excel con `df.to_excel()`**"
      ],
      "metadata": {
        "id": "ib4PMr3P7o60"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Usaremos df_excel_hoja1\n",
        "print(\"DataFrame que vamos a 'guardar' en Excel:\")\n",
        "print(df_excel_hoja1)\n",
        "\n",
        "# Simular la escritura a un buffer de bytes\n",
        "buffer_escritura_excel = io.BytesIO()\n",
        "df_excel_hoja1.to_excel(buffer_escritura_excel, sheet_name='MisDatos', index=False)\n",
        "# Para verificar, tendríamos que leer este buffer_escritura_excel de nuevo,\n",
        "# pero la idea es que esto crearía un archivo Excel.\n",
        "print(\"\\n(Simulación) DataFrame 'guardado' en un buffer de Excel.\")\n",
        "print(\"Parámetros comunes: sheet_name, index=False, header=True (por defecto)\")\n",
        "\n",
        "# Para escribir múltiples DataFrames en diferentes hojas de un mismo archivo Excel:\n",
        "# with pd.ExcelWriter('mi_archivo_multihoja.xlsx', engine='openpyxl') as writer:\n",
        "#     df1.to_excel(writer, sheet_name='Datos_Set_1', index=False)\n",
        "#     df2.to_excel(writer, sheet_name='Datos_Set_2', index=False)\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "id": "W9C56dhh7yL9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# **Funciones Útiles y Operaciones Avanzadas.**\n",
        "\n",
        "Esta sección te mostrará algunas funcionalidades adicionales de Pandas que son extremadamente útiles para refinar tus análisis, trabajar con tipos de datos específicos y realizar transformaciones más complejas.\n",
        "\n"
      ],
      "metadata": {
        "id": "hF8mD-GL7zm8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Dataset para la sección \"\"\"\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# DataFrame de ejemplo para el Paso 8\n",
        "datos_avanzados = {\n",
        "    'ID_Venta': range(1, 11),\n",
        "    'Fecha_Venta': pd.to_datetime([\n",
        "        '2025-01-15 10:30:00', '2025-01-15 14:45:00', '2025-01-16 09:15:00',\n",
        "        '2025-01-16 16:00:00', '2025-01-17 11:00:00', '2025-01-17 11:00:00', # Fecha duplicada\n",
        "        '2025-02-01 10:00:00', '2025-02-01 12:30:00', '2025-02-02 15:00:00',\n",
        "        '2025-02-02 18:00:00'\n",
        "    ]),\n",
        "    'Vendedor': ['Ana', 'Luis', 'Ana', 'Carlos', 'Luis', 'Luis', 'Ana', 'Carlos', 'Luis', 'Ana'],\n",
        "    'Producto': [\n",
        "        'Laptop MODELO Z', 'Mouse Inalámbrico X', 'Teclado Mecánico Y', 'Monitor 24\" HD',\n",
        "        'Laptop MODELO Z', 'Webcam Pro 1080p', 'SSD Externo 1TB', 'Mouse Inalámbrico X',\n",
        "        'Monitor 24\" HD', 'Teclado Mecánico Y'\n",
        "    ],\n",
        "    'Region': ['Norte', 'Sur', 'Norte', 'Este', 'Sur', 'Sur', 'Norte', 'Este', 'Sur', 'Norte'],\n",
        "    'Cantidad': [1, 2, 1, 1, 1, 3, 1, 1, 2, 2],\n",
        "    'Precio_Unitario': [1200, 25, 70, 150, 1200, 50, 80, 25, 150, 70],\n",
        "    'Cliente_ID': ['C001', 'C002', 'C003', 'C001', 'C004', 'C004', 'C005', 'C002', 'C001', 'C003']\n",
        "}\n",
        "df_adv = pd.DataFrame(datos_avanzados)\n",
        "df_adv['Total_Venta'] = df_adv['Cantidad'] * df_adv['Precio_Unitario']\n",
        "\n",
        "print(\"DataFrame original para el Paso 8:\")\n",
        "print(df_adv)\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "id": "g80eqIIT8YNp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Manejo de Fechas y Horas (`.dt accessor`)**\n",
        "\n",
        "Si tienes una columna de tipo datetime (puedes convertirla con `pd.to_datetime())`, el accesor `.dt` te permite extraer muchas partes de la fecha/hora."
      ],
      "metadata": {
        "id": "8cifZTmM8dTN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- Manejo de Fechas y Horas ---\")\n",
        "# Asegurarse de que 'Fecha_Venta' es datetime (ya lo hicimos al crear)\n",
        "# df_adv['Fecha_Venta'] = pd.to_datetime(df_adv['Fecha_Venta'])\n",
        "\n",
        "df_adv['Año_Venta'] = df_adv['Fecha_Venta'].dt.year\n",
        "df_adv['Mes_Venta'] = df_adv['Fecha_Venta'].dt.month\n",
        "df_adv['Dia_Venta'] = df_adv['Fecha_Venta'].dt.day\n",
        "df_adv['Hora_Venta'] = df_adv['Fecha_Venta'].dt.hour\n",
        "df_adv['Dia_Semana_Num'] = df_adv['Fecha_Venta'].dt.dayofweek # Lunes=0, Domingo=6\n",
        "df_adv['Dia_Semana_Nombre'] = df_adv['Fecha_Venta'].dt.day_name(locale='es_ES.UTF-8') # Especificar locale para nombres en español\n",
        "df_adv['Nombre_Mes'] = df_adv['Fecha_Venta'].dt.month_name(locale='es_ES.UTF-8')\n",
        "\n",
        "print(\"DataFrame con columnas de fecha extraídas:\")\n",
        "print(df_adv[['Fecha_Venta', 'Año_Venta', 'Nombre_Mes', 'Dia_Venta', 'Hora_Venta', 'Dia_Semana_Nombre']].head())\n",
        "\n",
        "# Formatear fecha como string\n",
        "df_adv['Fecha_Formateada'] = df_adv['Fecha_Venta'].dt.strftime('%d/%m/%Y %H:%M')\n",
        "print(\"\\nFecha formateada:\")\n",
        "print(df_adv[['Fecha_Venta', 'Fecha_Formateada']].head())\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "id": "imFOm9cw8pTp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Visualización Básica con `.plot()`**\n",
        "\n",
        "Pandas se integra con Matplotlib para ofrecer funciones de trazado rápidas directamente desde DataFrames y Series. Para gráficos más personalizados, usarías Matplotlib o Seaborn directamente."
      ],
      "metadata": {
        "id": "0DeEEJK58s7p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Es posible que necesites instalar matplotlib: pip install matplotlib\n",
        "print(\"--- Visualización Básica ---\")\n",
        "# Para que los gráficos se muestren en entornos como Jupyter, a veces se necesita:\n",
        "# %matplotlib inline\n",
        "\n",
        "# Ventas totales por vendedor\n",
        "ventas_por_vendedor = df_adv.groupby('Vendedor')['Total_Venta'].sum()\n",
        "print(\"\\nVentas totales por vendedor:\")\n",
        "print(ventas_por_vendedor)\n",
        "\n",
        "# Gráfico de barras de ventas por vendedor\n",
        "# En un script .py, necesitarías plt.show() de matplotlib.pyplot\n",
        "# En Colab/Jupyter, el gráfico suele aparecer automáticamente.\n",
        "print(\"\\nGenerando gráfico de barras (puede no mostrarse en todos los entornos de texto):\")\n",
        "grafico_vendedores = ventas_por_vendedor.plot(kind='bar', title='Ventas Totales por Vendedor')\n",
        "# Para mostrarlo si no aparece:\n",
        "# import matplotlib.pyplot as plt\n",
        "# plt.ylabel(\"Total Ventas\")\n",
        "# plt.tight_layout()\n",
        "# plt.show()\n",
        "\n",
        "# Gráfico de línea de cantidad de productos vendidos a lo largo del tiempo (simplificado)\n",
        "# df_adv.sort_values('Fecha_Venta')['Cantidad'].plot(kind='line', figsize=(10,5), title='Cantidad Vendida en el Tiempo')\n",
        "# plt.show()\n",
        "\n",
        "print(\"Los gráficos se generan; su visualización depende del entorno.\")\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "id": "KXvZYahr9GC6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Manejo de Datos Duplicados**\n",
        "\n",
        "* **`.duplicated()`:** Devuelve una Serie booleana indicando si cada fila es un duplicado de una fila anterior.\n",
        "* **`.drop_duplicates()`:** Elimina filas duplicadas."
      ],
      "metadata": {
        "id": "t2DFVkna9KhW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- Manejo de Datos Duplicados ---\")\n",
        "# Añadamos una fila duplicada para el ejemplo\n",
        "fila_duplicada_datos = {\n",
        "    'ID_Venta': 11, 'Fecha_Venta': pd.to_datetime('2025-01-17 11:00:00'),\n",
        "    'Vendedor': 'Luis', 'Producto': 'Laptop MODELO Z', 'Region': 'Sur',\n",
        "    'Cantidad': 1, 'Precio_Unitario': 1200, 'Cliente_ID': 'C004',\n",
        "    'Total_Venta': 1200 # Calculado: 1 * 1200\n",
        "}\n",
        "df_con_duplicados = pd.concat([df_adv, pd.DataFrame([fila_duplicada_datos])], ignore_index=True)\n",
        "\n",
        "\n",
        "print(f\"Número de filas antes de añadir duplicado: {len(df_adv)}\")\n",
        "print(f\"Número de filas después de añadir duplicado: {len(df_con_duplicados)}\")\n",
        "\n",
        "# Revisar duplicados basados en todas las columnas\n",
        "duplicados_todos = df_con_duplicados.duplicated(keep=False) # keep=False marca todos los duplicados como True\n",
        "print(\"\\nFilas completamente duplicadas (keep=False):\")\n",
        "print(df_con_duplicados[duplicados_todos])\n",
        "\n",
        "# Revisar duplicados basados en un subconjunto de columnas\n",
        "duplicados_subset = df_con_duplicados.duplicated(subset=['Fecha_Venta', 'Vendedor', 'Producto'], keep='first')\n",
        "print(\"\\nBoolean Series para duplicados en subset (keep='first'):\")\n",
        "print(duplicados_subset.tail()) # Muestra el final para ver el nuevo duplicado\n",
        "\n",
        "# Eliminar duplicados, manteniendo la primera aparición\n",
        "df_sin_duplicados = df_con_duplicados.drop_duplicates(keep='first')\n",
        "print(f\"\\nNúmero de filas después de drop_duplicates: {len(df_sin_duplicados)}\")\n",
        "\n",
        "# Eliminar duplicados basados en un subconjunto, manteniendo la última aparición\n",
        "df_sin_duplicados_subset_last = df_con_duplicados.drop_duplicates(\n",
        "    subset=['Fecha_Venta', 'Vendedor', 'Producto'],\n",
        "    keep='last'\n",
        ")\n",
        "print(f\"Número de filas después de drop_duplicates en subset (keep='last'): {len(df_sin_duplicados_subset_last)}\")\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "id": "njSKLnmz-OOz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Pivot Tables con `.pivot_table()`**\n",
        "\n",
        "Las tablas pivote son una forma de resumir y reorganizar datos. Son similares a las tablas dinámicas de Excel."
      ],
      "metadata": {
        "id": "YH0j47XW-amz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- Pivot Tables ---\")\n",
        "# Queremos ver el total de ventas ('Total_Venta') por cada 'Vendedor' (índice/filas)\n",
        "# y para cada 'Region' (columnas)\n",
        "tabla_pivote_ventas = pd.pivot_table(\n",
        "    df_adv,\n",
        "    values='Total_Venta',   # Valores a agregar\n",
        "    index='Vendedor',       # Filas de la tabla pivote\n",
        "    columns='Region',       # Columnas de la tabla pivote\n",
        "    aggfunc='sum',          # Función de agregación (suma por defecto es media si no se especifica)\n",
        "    fill_value=0            # Rellenar NaN (donde no hay combinación) con 0\n",
        ")\n",
        "print(\"Tabla Pivote: Suma de 'Total_Venta' por Vendedor y Región:\")\n",
        "print(tabla_pivote_ventas)\n",
        "\n",
        "# Múltiples valores y funciones de agregación\n",
        "tabla_pivote_compleja = pd.pivot_table(\n",
        "    df_adv,\n",
        "    index='Vendedor',\n",
        "    columns='Region',\n",
        "    values=['Cantidad', 'Total_Venta'],\n",
        "    aggfunc={'Cantidad': 'sum', 'Total_Venta': [np.sum, np.mean, 'count']}, # 'count' es de las ventas\n",
        "    fill_value=0\n",
        ")\n",
        "print(\"\\nTabla Pivote Compleja:\")\n",
        "print(tabla_pivote_compleja)\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "id": "tkWm2FOP-nhv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Uso de `apply()`, `map()`**\n",
        "\n",
        "* **`Series.apply(func)`:** Aplica una función a cada elemento de una Serie. La función puede ser una lambda o una función definida.\n",
        "* **`Series.map(arg)`:** Similar a apply para transformaciones elemento a elemento, pero también puede tomar un diccionario o una Serie para mapear valores.\n",
        "* **`DataFrame.apply(func, axis=0|1)`:** Aplica una función a lo largo de un eje del DataFrame.\n",
        "  * **`axis=0:`** Aplica la función a cada columna (la función recibe la columna como una Serie).\n",
        "  * **`axis=1:`** Aplica la función a cada fila (la función recibe la fila como una Serie).\n",
        "* **`DataFrame.map(func)` (Pandas 2.1.0+):** Aplica una función elemento por elemento a todo el DataFrame.\n",
        "  * *Antes de Pandas 2.1.0*, `applymap(func)` hacía esto, pero ahora está deprecado. La alternativa era `df.apply(lambda col: col.map(func))` o bucles."
      ],
      "metadata": {
        "id": "jgESEJwM-pgB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- Uso de apply, map ---\")\n",
        "# Series.apply()\n",
        "df_adv['Longitud_Nombre_Producto'] = df_adv['Producto'].apply(len)\n",
        "print(\"Longitud de nombres de producto (Series.apply):\")\n",
        "print(df_adv[['Producto', 'Longitud_Nombre_Producto']].head())\n",
        "\n",
        "# Series.map() para reemplazar valores\n",
        "mapeo_region = {'Norte': 'N', 'Sur': 'S', 'Este': 'E', 'Oeste': 'O'}\n",
        "df_adv['Region_Abrev'] = df_adv['Region'].map(mapeo_region)\n",
        "print(\"\\nRegiones abreviadas (Series.map):\")\n",
        "print(df_adv[['Region', 'Region_Abrev']].head())\n",
        "\n",
        "# DataFrame.apply() para operar por filas (axis=1)\n",
        "def categoria_precio(fila):\n",
        "    if pd.isna(fila['Precio_Unitario']):\n",
        "        return \"Desconocido\"\n",
        "    if fila['Precio_Unitario'] > 1000:\n",
        "        return \"Muy Caro\"\n",
        "    elif fila['Precio_Unitario'] > 100:\n",
        "        return \"Caro\"\n",
        "    else:\n",
        "        return \"Barato\"\n",
        "\n",
        "df_adv['Categoria_Precio_Fila'] = df_adv.apply(categoria_precio, axis=1)\n",
        "print(\"\\nCategoría de precio basada en la fila (DataFrame.apply axis=1):\")\n",
        "print(df_adv[['Producto', 'Precio_Unitario', 'Categoria_Precio_Fila']].head())\n",
        "\n",
        "# DataFrame.map() (Pandas 2.1.0+) para aplicar una función a cada elemento\n",
        "# Supongamos que queremos añadir \" (verificado)\" a todos los productos si son strings\n",
        "# Como 'Producto' es la única columna de strings relevante, lo haremos en esa serie.\n",
        "# Si tuvieramos múltiples columnas de texto:\n",
        "# df_texto = df_adv.select_dtypes(include='object')\n",
        "# df_texto_verificado = df_texto.map(lambda x: f\"{x} (verificado)\" if isinstance(x, str) else x)\n",
        "# print(df_texto_verificado.head())\n",
        "# Para este ejemplo, nos enfocamos en una columna:\n",
        "df_adv['Producto_Verificado'] = df_adv['Producto'].map(lambda x: f\"{x} (verificado)\")\n",
        "print(\"\\nProducto verificado (Series.map):\")\n",
        "print(df_adv[['Producto', 'Producto_Verificado']].head())\n",
        "\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "id": "AuHBre1j_rye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Trabajo con Datos de Texto `(.str accessor)`**\n",
        "\n",
        "Para Series que contienen strings, el accesor `.str` te da acceso a un montón de métodos vectorizados para manipular texto."
      ],
      "metadata": {
        "id": "4n9jmcBQ_tID"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- Métodos de String (.str) ---\")\n",
        "df_adv['Producto_Minusculas'] = df_adv['Producto'].str.lower()\n",
        "print(\"Producto en minúsculas:\")\n",
        "print(df_adv[['Producto', 'Producto_Minusculas']].head())\n",
        "\n",
        "df_adv['Contiene_Laptop'] = df_adv['Producto'].str.contains('Laptop', case=False) # case=False ignora may/min\n",
        "print(\"\\n¿Producto contiene 'Laptop'?:\")\n",
        "print(df_adv[['Producto', 'Contiene_Laptop']].head())\n",
        "\n",
        "# Extraer el modelo si el producto es una laptop\n",
        "# Esto usa expresiones regulares, que es un tema más avanzado.\n",
        "df_adv['Modelo_Extraido'] = df_adv['Producto'].str.extract(r'MODELO (\\w+)', expand=True)\n",
        "print(\"\\nModelo extraído de 'Laptop MODELO Z':\")\n",
        "print(df_adv[['Producto', 'Modelo_Extraido']].head())\n",
        "\n",
        "# Dividir el nombre del producto en palabras\n",
        "# df_adv['Palabras_Producto'] = df_adv['Producto'].str.split(' ')\n",
        "# print(\"\\nPalabras del producto:\")\n",
        "# print(df_adv[['Producto', 'Palabras_Producto']].head())\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "id": "QZtd_BLYAGgi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}